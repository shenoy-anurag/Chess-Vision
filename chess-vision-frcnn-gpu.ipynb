{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-24T04:52:46.527913Z","iopub.execute_input":"2022-04-24T04:52:46.528234Z","iopub.status.idle":"2022-04-24T04:52:46.902134Z","shell.execute_reply.started":"2022-04-24T04:52:46.528153Z","shell.execute_reply":"2022-04-24T04:52:46.901359Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/chess-object-detection/chess-pieces-dataset/valid/d114edc5cb4cae0ceb2f152afd15f57d_jpg.rf.fa2760e7c7663ed523265248c14b35ea.xml\n/kaggle/input/chess-object-detection/chess-pieces-dataset/test/IMG_0159_JPG.rf.f0d34122f8817d538e396b04f2b70d33.jpg\n/kaggle/input/chess-object-detection/chess-pieces-dataset/train/IMG_0167_JPG.rf.51e844ec48d744ea0d541c3978e68c8f.xml\n","output_type":"stream"}]},{"cell_type":"code","source":"# datasets/image\n\nimport imageio\nfrom PIL import Image\nimport numpy as np\n\ndef _compute_scale_factor(original_width, original_height, min_dimension_pixels):\n  if not min_dimension_pixels:\n    return 1.0\n  if original_width > original_height:\n    scale_factor = min_dimension_pixels / original_height\n  else:\n    scale_factor = min_dimension_pixels / original_width\n  return scale_factor\n\ndef _preprocess_vgg16(image_data):\n  image_data = image_data[:, :, ::-1]           # RGB -> BGR\n  image_data[:, :, 0] -= 103.939                # ImageNet B mean\n  image_data[:, :, 1] -= 116.779                # ImageNet G mean\n  image_data[:, :, 2] -= 123.680                # ImageNet R mean \n  return image_data\n\ndef load_image(url, min_dimension_pixels = None, horizontal_flip = False):\n  \"\"\"\n  Loads and preprocesses an image for use with VGG-16, which consists of\n  converting RGB to BGR and subtracting ImageNet dataset means from each\n  component. The image can be resized so that the minimum dimension is a\n  defined size, as recommended by Faster R-CNN. \n\n  Parameters\n  ----------\n  url : str\n    URL (local or remote file) to load.\n  min_dimension_pixels : int\n    If not None, specifies the size in pixels of the smaller side of the image.\n    The other side is scaled proportionally.\n  horizontal_flip : bool\n    Whether to flip the image horizontally.\n\n  Returns\n  -------\n  np.ndarray, PIL.Image, float, Tuple[int, int, int]\n    Image pixels as float32, shaped as (channels, height, width); an image\n    object suitable for drawing and visualization; scaling factor applied to\n    the image dimensions; and the original image shape.\n  \"\"\"\n  data = imageio.imread(url, pilmode = \"RGB\")\n  image = Image.fromarray(data, mode = \"RGB\")\n  original_width, original_height = image.width, image.height\n  if horizontal_flip:\n    image = image.transpose(method = Image.FLIP_LEFT_RIGHT)\n  if min_dimension_pixels is not None:\n    scale_factor = _compute_scale_factor(original_width = image.width, original_height = image.height, min_dimension_pixels = min_dimension_pixels)\n    width = int(image.width * scale_factor)\n    height = int(image.height * scale_factor)\n    image = image.resize((width, height), resample = Image.BILINEAR)\n  else:\n    scale_factor = 1.0\n  image_data = np.array(image).astype(np.float32)\n  image_data = _preprocess_vgg16(image_data = image_data)\n  return image_data, image, scale_factor, (image_data.shape[0], original_height, original_width)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:46.904059Z","iopub.execute_input":"2022-04-24T04:52:46.904542Z","iopub.status.idle":"2022-04-24T04:52:46.952365Z","shell.execute_reply.started":"2022-04-24T04:52:46.904504Z","shell.execute_reply":"2022-04-24T04:52:46.951648Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# datasets/training_sample\n\nfrom dataclasses import dataclass\nimport numpy as np\nfrom PIL import Image\nfrom typing import List\nfrom typing import Tuple\n\n\n@dataclass\nclass Box:\n  class_index: int\n  class_name: str\n  corners: np.ndarray\n  \n  def __repr__(self):\n    return \"[class=%s (%f,%f,%f,%f)]\" % (self.class_name, self.corners[0], self.corners[1], self.corners[2], self.corners[3])\n\n  def __str__(self):\n    return repr(self)\n\n@dataclass\nclass TrainingSample:\n  anchor_map:                 np.ndarray                # shape (feature_map_height,feature_map_width,num_anchors*4), with each anchor as [center_y,center_x,height,width]\n  anchor_valid_map:           np.ndarray                # shape (feature_map_height,feature_map_width,num_anchors), indicating which anchors are valid (do not cross image boundaries)\n  gt_rpn_map:                 np.ndarray                # TODO: describe me\n  gt_rpn_object_indices:      List[Tuple[int,int,int]]  # list of (y,x,k) coordinates of anchors in gt_rpn_map that are labeled as object\n  gt_rpn_background_indices:  List[Tuple[int,int,int]]  # list of (y,x,k) coordinates of background anchors\n  gt_boxes:                   List[Box]                 # list of ground-truth boxes, scaled\n  image_data:                 np.ndarray                # shape (3,height,width), pre-processed and scaled to size expected by model\n  image:                      Image                     # PIL image data (for debug rendering), scaled\n  filepath:                   str                       # file path of image\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:46.953722Z","iopub.execute_input":"2022-04-24T04:52:46.954201Z","iopub.status.idle":"2022-04-24T04:52:46.964336Z","shell.execute_reply.started":"2022-04-24T04:52:46.954163Z","shell.execute_reply":"2022-04-24T04:52:46.963640Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# datasets/voc.py\n\nfrom dataclasses import dataclass\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport random\nimport xml.etree.ElementTree as ET\nfrom typing import List\nfrom typing import Tuple\n\n\nclass Dataset:\n    \"\"\"\n    A VOC dataset iterator for a particular split (train, val, etc.)\n    \"\"\"\n\n    num_classes = 15\n    class_index_to_name = {\n        0: 'background', 1: 'pieces', 2: 'bishop', 3: 'black-bishop', 4: 'black-king', 5: 'black-knight', 6: 'black-pawn',\n        7: 'black-queen', 8: 'black-rook', 9: 'white-bishop', 10: 'white-king', 11: 'white-knight', 12: 'white-pawn',\n        13: 'white-queen', 14: 'white-rook'\n    }\n\n    def __init__(self, split, dir=\"dataset/chess-pieces-dataset\", feature_pixels=16, augment=True, shuffle=True,\n                 allow_difficult=False, cache=True):\n        \"\"\"\n        Parameters\n        ----------\n        split : str\n          Dataset split to load: train, val, or trainval.\n        dir : str\n          Root directory of dataset.\n        feature_pixels : int\n          Size of each cell in the Faster R-CNN feature map (i.e., VGG-16 feature\n          extractor output) in image pixels. This is the separation distance\n          between anchors.\n        augment : bool\n          Whether to randomly augment (horizontally flip) images during iteration\n          with 50% probability.\n        shuffle : bool\n          Whether to shuffle the dataset each time it is iterated.\n        allow_difficult : bool\n          Whether to include ground truth boxes that are marked as \"difficult\".\n        cache : bool\n          Whether to training samples in memory after first being generated.\n        \"\"\"\n        if not os.path.exists(dir):\n            raise FileNotFoundError(\"Dataset directory does not exist: %s\" % dir)\n        self.split = split\n        self._dir = dir\n        # self.class_index_to_name = self._get_classes()\n        # self.class_index_to_name = self._add_background_class(self.class_index_to_name)\n        self.class_name_to_index = {class_name: class_index for (class_index, class_name) in\n                                    self.class_index_to_name.items()}\n        self.num_classes = len(self.class_index_to_name)\n        assert self.num_classes == Dataset.num_classes, \"Dataset does not have the expected number of classes (found %d but expected %d)\" % (\n            self.num_classes, Dataset.num_classes)\n        # assert self.class_index_to_name == Dataset.class_index_to_name, \"Dataset does not have the expected class mapping\"\n        self._filepaths = self._get_filepaths()\n        self.num_samples = len(self._filepaths)\n        self._gt_boxes_by_filepath = self._get_ground_truth_boxes(filepaths=self._filepaths,\n                                                                  allow_difficult=allow_difficult)\n        self._i = 0\n        self._iterable_filepaths = self._filepaths.copy()\n        self._feature_pixels = feature_pixels\n        self._augment = augment\n        self._shuffle = shuffle\n        self._cache = cache\n        self._unaugmented_cached_sample_by_filepath = {}\n        self._augmented_cached_sample_by_filepath = {}\n\n    def __iter__(self):\n        self._i = 0\n        if self._shuffle:\n            random.shuffle(self._iterable_filepaths)\n        return self\n\n    def __next__(self):\n        if self._i >= len(self._iterable_filepaths):\n            raise StopIteration\n\n        # Next file to load\n        filepath = self._iterable_filepaths[self._i]\n        self._i += 1\n\n        # Augment?\n        flip = random.randint(0, 1) != 0 if self._augment else 0\n        cached_sample_by_filepath = self._augmented_cached_sample_by_filepath if flip else self._unaugmented_cached_sample_by_filepath\n\n        # Load and, if caching, write back to cache\n        if filepath in cached_sample_by_filepath:\n            sample = cached_sample_by_filepath[filepath]\n        else:\n            sample = self._generate_training_sample(filepath=filepath, flip=flip)\n        if self._cache:\n            cached_sample_by_filepath[filepath] = sample\n\n        # Return the sample\n        return sample\n\n    def _generate_training_sample(self, filepath, flip):\n        # Load and preprocess the image\n        scaled_image_data, scaled_image, scale_factor, original_shape = load_image(url=filepath,\n                                                                                         min_dimension_pixels=600,\n                                                                                         horizontal_flip=flip)\n        _, original_height, original_width = original_shape\n\n        # Scale ground truth boxes to new image size\n        scaled_gt_boxes = []\n        for box in self._gt_boxes_by_filepath[filepath]:\n            if flip:\n                corners = np.array([\n                    box.corners[0],\n                    original_width - 1 - box.corners[3],\n                    box.corners[2],\n                    original_width - 1 - box.corners[1]\n                ])\n            else:\n                corners = box.corners\n            scaled_box = Box(\n                class_index=box.class_index,\n                class_name=box.class_name,\n                corners=corners * scale_factor\n            )\n            scaled_gt_boxes.append(scaled_box)\n\n        # Generate anchor maps and RPN truth map\n        anchor_map, anchor_valid_map = generate_anchor_maps(image_shape=scaled_image_data.shape,\n                                                                    feature_pixels=self._feature_pixels)\n        gt_rpn_map, gt_rpn_object_indices, gt_rpn_background_indices = generate_rpn_map(anchor_map=anchor_map,\n                                                                                                anchor_valid_map=anchor_valid_map,\n                                                                                                gt_boxes=scaled_gt_boxes)\n\n        # Return sample\n        return TrainingSample(\n            anchor_map=anchor_map,\n            anchor_valid_map=anchor_valid_map,\n            gt_rpn_map=gt_rpn_map,\n            gt_rpn_object_indices=gt_rpn_object_indices,\n            gt_rpn_background_indices=gt_rpn_background_indices,\n            gt_boxes=scaled_gt_boxes,\n            image_data=scaled_image_data,\n            image=scaled_image,\n            filepath=filepath\n        )\n\n    def _get_classes(self):\n        imageset_dir = os.path.join(self._dir, \"ImageSets\", \"Main\")\n        classes = set(\n            [os.path.basename(path).split(\"_\")[0] for path in Path(imageset_dir).glob(\"*_\" + self.split + \".txt\")])\n        assert len(classes) > 0, \"No classes found in ImageSets/Main for '%s' split\" % self.split\n        class_index_to_name = {(1 + v[0]): v[1] for v in enumerate(sorted(classes))}\n        class_index_to_name[0] = \"background\"\n        return class_index_to_name\n\n    def _add_background_class(self, class_index_to_name: dict):\n        class_index_to_name = {(1 + k): v for k, v in class_index_to_name.items()}\n        class_index_to_name[0] = \"background\"\n        return class_index_to_name\n\n    def _get_filepaths(self):\n        image_paths = []\n        for root, dirs, _ in os.walk(self._dir):\n            for dir in dirs:\n                if self.split == dir:\n                    folder_path = os.path.join(root, dir)\n                    for _, _, files in os.walk(folder_path):\n                        for file in files:\n                            if file.rsplit(\".\")[-1] == \"jpg\":\n                                image_paths.append(os.path.join(root, self.split, file))\n        return image_paths\n        # Debug: 20 chess training images:\n        image_paths = [\n            '3bab0eaaeb63a2ac9ae4942df4006a25_jpg.rf.8fd1c7b01ae630cdb96546469e0c742d.jpg',\n            '3bab0eaaeb63a2ac9ae4942df4006a25_jpg.rf.b78947d5207c15119ee81058a1b75c1e.jpg',\n            '3161933dffedf8a859d6623a99492c53_jpg.rf.b3cca32040dcb031002296f83298f3d1.jpg',\n            '254f92b18b2a81f88b85e7aed3cabc61_jpg.rf.a55e3d26992b9f4d43e7f317a078689b.jpg',\n            'IMG_0291_JPG.rf.d2ba6353082aa25c15708824c08dfb27.jpg',\n            '5758322233deed7ae7adc23536db2a4f_jpg.rf.469940331ca0c0fbabd2eaad8348ed71.jpg',\n            '389b4c47568c78c44df11dbb1377ffea_jpg.rf.0185f6bf38d82f7cbf9365edd7b2bfc7.jpg',\n            '4894f034a55eaa9252cd261a62b11d27_jpg.rf.e153d650cc91ee8985dbc0f9b5050e98.jpg',\n            '4894f034a55eaa9252cd261a62b11d27_jpg.rf.bcd60bd54187dbd564c6d84e8a4d3cb9.jpg',\n            'd079f4e77b2445abceca7534356db743_jpg.rf.e7fc6fdfea0d14dc4c82a6068b9e4159.jpg',\n            '8ff64b3f770bfe96bdffc629efd16460_jpg.rf.7b4792b9f562b28d55342586be82fe91.jpg',\n            'ddad9dc4d945006d66f5349d64498559_jpg.rf.48e7a4d1dbc55402801f6f3eb2515561.jpg',\n            '1728cd731489df8bb8e0396e178fe393_jpg.rf.cf3127987c30548d691295953a2326db.jpg',\n            '02f0931b536dfba10affc3231a3d64fb_jpg.rf.087fbe5ea178dd757f4eb065ae5cf941.jpg',\n            '4894f034a55eaa9252cd261a62b11d27_jpg.rf.ec15ec6e91a0367ded74d29495beadca.jpg',\n            'f041d3171dfe3137390c85fc5437e447_jpg.rf.3020fee02bc4def16c99bed406ad8671.jpg',\n            '03886821377011fec599e8fa12d86e89_jpg.rf.7ec3f29be4f3793b35a2c4a9880d831c.jpg',\n            '9146a6989dac08f1769e677064ebfb49_jpg.rf.f479d3177bde0b8beb172fcd798971f2.jpg',\n            'a9768de3fceeeae2618f362870fb9a88_jpg.rf.444b950f0b329aa6e7ed17a86383606d.jpg',\n            '673bcd0d44f495fbe9dd88d5cacfceb3_jpg.rf.3b647f8c3bb9f3fc64a0d0edf806f691.jpg',\n            'IMG_0166_JPG.rf.866e83ca31acd30da2673fcb7e2abbfe.jpg',\n        ]\n        return [os.path.join(self._dir, self.split, path) for path in image_paths]\n\n    def _get_ground_truth_boxes(self, filepaths, allow_difficult):\n        gt_boxes_by_filepath = {}\n        for filepath in filepaths:\n            basename = os.path.splitext(os.path.basename(filepath))[0]\n            annotation_file = os.path.join(self._dir, self.split, basename) + \".xml\"\n            tree = ET.parse(annotation_file)\n            root = tree.getroot()\n            assert tree != None, \"Failed to parse %s\" % annotation_file\n            assert len(root.findall(\"size\")) == 1\n            size = root.find(\"size\")\n            assert len(size.findall(\"depth\")) == 1\n            depth = int(size.find(\"depth\").text)\n            assert depth == 3\n            boxes = []\n            for obj in root.findall(\"object\"):\n                assert len(obj.findall(\"name\")) == 1\n                assert len(obj.findall(\"bndbox\")) == 1\n                assert len(obj.findall(\"difficult\")) == 1\n                is_difficult = int(obj.find(\"difficult\").text) != 0\n                if is_difficult and not allow_difficult:\n                    continue  # ignore difficult examples unless asked to include them\n                class_name = obj.find(\"name\").text\n                bndbox = obj.find(\"bndbox\")\n                assert len(bndbox.findall(\"xmin\")) == 1\n                assert len(bndbox.findall(\"ymin\")) == 1\n                assert len(bndbox.findall(\"xmax\")) == 1\n                assert len(bndbox.findall(\"ymax\")) == 1\n                x_min = int(bndbox.find(\"xmin\").text) - 1  # convert to 0-based pixel coordinates\n                y_min = int(bndbox.find(\"ymin\").text) - 1\n                x_max = int(bndbox.find(\"xmax\").text) - 1\n                y_max = int(bndbox.find(\"ymax\").text) - 1\n                corners = np.array([y_min, x_min, y_max, x_max]).astype(np.float32)\n                box = Box(class_index=self.class_name_to_index[class_name], class_name=class_name, corners=corners)\n                boxes.append(box)\n            if len(boxes) == 0:\n                print(filepath)\n            assert len(boxes) > 0\n            gt_boxes_by_filepath[filepath] = boxes\n        return gt_boxes_by_filepath","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:46.965954Z","iopub.execute_input":"2022-04-24T04:52:46.966223Z","iopub.status.idle":"2022-04-24T04:52:47.009505Z","shell.execute_reply.started":"2022-04-24T04:52:46.966190Z","shell.execute_reply":"2022-04-24T04:52:47.008818Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# math_utils\n\ndef intersection_over_union(boxes1, boxes2):\n  \"\"\"\n  Computes intersection-over-union (IoU) for multiple boxes in parallel.\n\n  Parameters\n  ----------\n  boxes1 : np.ndarray\n    Box corners, shaped (N, 4), with each box as (y1, x1, y2, x2).\n  boxes2 : np.ndarray\n    Box corners, shaped (M, 4).\n\n  Returns\n  -------\n  np.ndarray\n    IoUs for each pair of boxes in boxes1 and boxes2, shaped (N, M).\n  \"\"\"\n  top_left_point = np.maximum(boxes1[:,None,0:2], boxes2[:,0:2])                                  # (N,1,2) and (M,2) -> (N,M,2) indicating top-left corners of box pairs\n  bottom_right_point = np.minimum(boxes1[:,None,2:4], boxes2[:,2:4])                              # \"\" bottom-right corners \"\"\n  well_ordered_mask = np.all(top_left_point < bottom_right_point, axis = 2)                       # (N,M) indicating whether top_left_x < bottom_right_x and top_left_y < bottom_right_y (meaning boxes may intersect)\n  intersection_areas = well_ordered_mask * np.prod(bottom_right_point - top_left_point, axis = 2) # (N,M) indicating intersection area (bottom_right_x - top_left_x) * (bottom_right_y - top_left_y)\n  areas1 = np.prod(boxes1[:,2:4] - boxes1[:,0:2], axis = 1)                                       # (N,) indicating areas of boxes1\n  areas2 = np.prod(boxes2[:,2:4] - boxes2[:,0:2], axis = 1)                                       # (M,) indicating areas of boxes2\n  union_areas = areas1[:,None] + areas2 - intersection_areas                                      # (N,1) + (M,) - (N,M) = (N,M), union areas of both boxes\n  epsilon = 1e-7\n  return intersection_areas / (union_areas + epsilon)\n\ndef tf_intersection_over_union(boxes1, boxes2):\n  \"\"\"\n  Equivalent of intersection_over_union() but operates on tf.Tensors and\n  produces a TensorFlow graph suitable for use in a model. This code borrowed\n  from Matterport's MaskRCNN implementation:\n  https://github.com/matterport/Mask_RCNN\n\n  Parameters\n  ----------\n  boxes1: tf.Tensor\n    Box corners, shaped (N,4), with each box as (y1, x1, y2, x2).\n  boxes2: tf.Tensor\n    Box corners, shaped (M,4).\n\n  Returns\n  -------\n  tf.Tensor\n    Tensor of shape (N, M) containing IoU score between each pair of boxes.\n  \"\"\"\n  # 1. Tile boxes2 and repeat boxes1. This allows us to compare\n  # every boxes1 against every boxes2 without loops.\n  # TF doesn't have an equivalent to np.repeat() so simulate it\n  # using tf.tile() and tf.reshape.\n  b1 = tf.reshape(tf.tile(tf.expand_dims(boxes1, 1),\n                          [1, 1, tf.shape(boxes2)[0]]), [-1, 4])\n  b2 = tf.tile(boxes2, [tf.shape(boxes1)[0], 1])\n  # 2. Compute intersections\n  b1_y1, b1_x1, b1_y2, b1_x2 = tf.split(b1, 4, axis=1)\n  b2_y1, b2_x1, b2_y2, b2_x2 = tf.split(b2, 4, axis=1)\n  y1 = tf.maximum(b1_y1, b2_y1)\n  x1 = tf.maximum(b1_x1, b2_x1)\n  y2 = tf.minimum(b1_y2, b2_y2)\n  x2 = tf.minimum(b1_x2, b2_x2)\n  intersection = tf.maximum(x2 - x1, 0) * tf.maximum(y2 - y1, 0)\n  # 3. Compute unions\n  b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n  b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n  union = b1_area + b2_area - intersection\n  # 4. Compute IoU and reshape to [boxes1, boxes2]\n  iou = intersection / union\n  overlaps = tf.reshape(iou, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\n  return overlaps\n\ndef convert_deltas_to_boxes(box_deltas, anchors, box_delta_means, box_delta_stds):\n  \"\"\"\n  Converts box deltas, which are in parameterized form (ty, tx, th, tw) as\n  described by the Fast R-CNN and Faster R-CNN papers, to boxes\n  (y1, x1, y2, x2). The anchors are the base boxes (e.g., RPN anchors or\n  proposals) that the deltas describe a modification to.\n\n  Parameters\n  ----------\n  box_deltas : np.ndarray\n    Box deltas with shape (N, 4). Each row is (ty, tx, th, tw).\n  anchors : np.ndarray\n    Corresponding anchors that the deltas are based upon, shaped (N, 4) with\n    each row being (center_y, center_x, height, width).\n  box_delta_means : np.ndarray\n    Mean ajustment to deltas, (4,), to be added after standard deviation\n    scaling and before conversion to actual box coordinates.\n  box_delta_stds : np.ndarray\n    Standard deviation adjustment to deltas, (4,). Box deltas are first\n    multiplied by these values.\n\n  Returns\n  -------\n  np.ndarray\n    Box coordinates, (N, 4), with each row being (y1, x1, y2, x2).\n  \"\"\"\n  box_deltas = box_deltas * box_delta_stds + box_delta_means\n  center = anchors[:,2:4] * box_deltas[:,0:2] + anchors[:,0:2]  # center_x = anchor_width * tx + anchor_center_x, center_y = anchor_height * ty + anchor_center_y\n  size = anchors[:,2:4] * np.exp(box_deltas[:,2:4])             # width = anchor_width * exp(tw), height = anchor_height * exp(th)\n  boxes = np.empty(box_deltas.shape)\n  boxes[:,0:2] = center - 0.5 * size                            # y1, x1\n  boxes[:,2:4] = center + 0.5 * size                            # y2, x2\n  return boxes\n\ndef tf_convert_deltas_to_boxes(box_deltas, anchors, box_delta_means, box_delta_stds):\n  \"\"\"\n  Equivalent of convert_deltas_to_boxes() but operates on tf.Tensors and\n  produces a TensorFlow graph suitable for use in a model.\n\n  Parameters\n  ----------\n  box_deltas : np.ndarray\n    Box deltas with shape (N, 4). Each row is (ty, tx, th, tw).\n  anchors : np.ndarray\n    Corresponding anchors that the deltas are based upon, shaped (N, 4) with\n    each row being (center_y, center_x, height, width).\n  box_delta_means : np.ndarray\n    Mean ajustment to deltas, (4,), to be added after standard deviation\n    scaling and before conversion to actual box coordinates.\n  box_delta_stds : np.ndarray\n    Standard deviation adjustment to deltas, (4,). Box deltas are first\n    multiplied by these values.\n\n  Returns\n  -------\n  tf.Tensor\n    Box coordinates, (N, 4), with each row being (y1, x1, y2, x2).\n  \"\"\"\n  box_deltas = box_deltas * box_delta_stds + box_delta_means\n  center = anchors[:,2:4] * box_deltas[:,0:2] + anchors[:,0:2]  # center_x = anchor_width * tx + anchor_center_x, center_y = anchor_height * ty + anchor_center_y\n  size = anchors[:,2:4] * tf.math.exp(box_deltas[:,2:4])        # width = anchor_width * exp(tw), height = anchor_height * exp(th)\n  boxes_top_left = center - 0.5 * size                          # y1, x1\n  boxes_bottom_right = center + 0.5 * size                      # y2, x2\n  boxes = tf.concat([ boxes_top_left, boxes_bottom_right ], axis = 1) # [ (N,2), (N,2) ] -> (N,4)\n  return boxes\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:47.013161Z","iopub.execute_input":"2022-04-24T04:52:47.013633Z","iopub.status.idle":"2022-04-24T04:52:47.126429Z","shell.execute_reply.started":"2022-04-24T04:52:47.013586Z","shell.execute_reply":"2022-04-24T04:52:47.125756Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# statistics\n\nfrom collections import defaultdict\n\n\nclass TrainingStatistics:\n  \"\"\"\n  Computes statistics per epoch.\n  \"\"\"\n  def __init__(self):\n    self.rpn_class_loss = float(\"inf\")\n    self.rpn_regression_loss = float(\"inf\")\n    self.detector_class_loss = float(\"inf\")\n    self.detector_regression_loss = float(\"inf\")\n    self._rpn_class_losses = []\n    self._rpn_regression_losses = []\n    self._detector_class_losses = []\n    self._detector_regression_losses = []\n\n  def on_training_step(self, losses):\n    \"\"\"\n    Call once per training iteration to aggregate losses.\n\n    Parameters\n    ----------\n    losses : models.faster_rcnn.FasterRCNNModel.Loss\n      Dataclass containing losses broken down by RPN and detector, and further\n      by classifier and regression loss. \n    \"\"\"\n    self._rpn_class_losses.append(losses[\"rpn_class_loss\"])\n    self._rpn_regression_losses.append(losses[\"rpn_regression_loss\"])\n    self._detector_class_losses.append(losses[\"detector_class_loss\"])\n    self._detector_regression_losses.append(losses[\"detector_regression_loss\"])\n    self.rpn_class_loss = np.mean(self._rpn_class_losses)\n    self.rpn_regression_loss = np.mean(self._rpn_regression_losses)\n    self.detector_class_loss = np.mean(self._detector_class_losses)\n    self.detector_regression_loss = np.mean(self._detector_regression_losses)\n\n  def get_progbar_postfix(self):\n    \"\"\"\n    Returns\n    -------\n    Dict[str, str]\n      A dictionary of labels and values suitable for use as a postfix object\n      for a tqdm progress bar.\n    \"\"\"\n    return { \n      \"rpn_class_loss\": \"%1.4f\" % self.rpn_class_loss,\n      \"rpn_regr_loss\": \"%1.4f\" % self.rpn_regression_loss,\n      \"detector_class_loss\": \"%1.4f\" % self.detector_class_loss,\n      \"detector_regr_loss\": \"%1.4f\" % self.detector_regression_loss,\n      \"total_loss\": \"%1.2f\" % (self.rpn_class_loss + self.rpn_regression_loss + self.detector_class_loss + self.detector_regression_loss)\n    }\n\n\nclass PrecisionRecallCurveCalculator:\n  \"\"\"\n  Collects data over the course of a validation pass and then computes\n  precision and recall (including mean average precision).\n  \"\"\"\n  def __init__(self):\n    # List of (confidence_score, correctness) by class for all images in dataset\n    self._unsorted_predictions_by_class_index = defaultdict(list)\n\n    # True number of objects by class for all images in dataset\n    self._object_count_by_class_index = defaultdict(int)\n\n  def _compute_correctness_of_predictions(self, scored_boxes_by_class_index, gt_boxes):\n    unsorted_predictions_by_class_index = {}\n    object_count_by_class_index = defaultdict(int)\n\n    # Count objects by class. We do this here because in case there are no\n    # predictions, we do not want to miscount the total number of objects.\n    for gt_box in gt_boxes:\n      object_count_by_class_index[gt_box.class_index] += 1\n\n    for class_index, scored_boxes in scored_boxes_by_class_index.items():\n      # Get the ground truth boxes corresponding to this class\n      gt_boxes_this_class = [ gt_box for gt_box in gt_boxes if gt_box.class_index == class_index ]\n\n      # Compute IoU of each box with each ground truth box and store as a list\n      # of tuples (iou, box_index, gt_box_index) by descending IoU\n      ious = []\n      for gt_idx in range(len(gt_boxes_this_class)):\n        for box_idx in range(len(scored_boxes)):\n          boxes1 = np.expand_dims(scored_boxes[box_idx][0:4], axis = 0) # convert single box (4,) to (1,4), as expected by parallel IoU function\n          boxes2 = np.expand_dims(gt_boxes_this_class[gt_idx].corners, axis = 0)\n          iou = intersection_over_union(boxes1 = boxes1, boxes2 = boxes2) \n          ious.append((iou, box_idx, gt_idx))\n      ious = sorted(ious, key = lambda iou: ious[0], reverse = True)  # sort descending by IoU\n      \n      # Vector that indicates whether a ground truth box has been detected\n      gt_box_detected = [ False ] * len(gt_boxes)\n\n      # Vector that indicates whether a prediction is a true positive (True) or\n      # false positive (False)\n      is_true_positive = [ False ] * len(scored_boxes)\n      \n      #\n      # Construct a list of prediction descriptions: (score, correct)\n      # Score is the confidence score of the predicted box and correct is\n      # whether it is a true positive (True) or false positive (False).\n      #\n      # A true positive is a prediction that has an IoU of > 0.5 and is\n      # also the highest-IoU prediction for a ground truth box. Predictions\n      # with IoU <= 0.5 or that do not have the highest IoU for any ground\n      # truth box are considered false positives.\n      #\n      iou_threshold = 0.5\n      for iou, box_idx, gt_idx in ious:\n        if iou <= iou_threshold:\n          continue\n        if is_true_positive[box_idx] or gt_box_detected[gt_idx]:\n          # The prediction and/or ground truth box have already been matched\n          continue\n        # We've got a true positive\n        is_true_positive[box_idx] = True\n        gt_box_detected[gt_idx] = True\n      # Construct the final array of prediction descriptions\n      unsorted_predictions_by_class_index[class_index] = [ (scored_boxes[i][4], is_true_positive[i]) for i in range(len(scored_boxes)) ]\n        \n    return unsorted_predictions_by_class_index, object_count_by_class_index\n\n  def add_image_results(self, scored_boxes_by_class_index, gt_boxes):\n    \"\"\"\n    Adds a detection result to the running tally. Should be called only once per\n    image in the dataset.\n\n    Parameters\n    ----------\n    scored_boxes_by_class_index : Dict[int, Tuple[float, float, float, float, float]]\n      Final detected boxes as lists of tuples, (y_min, x_min, y_max, x_max,\n      score), by class index. The score is the softmax output and is\n      interpreted as a confidence metric when sorting results for the mAP\n      calculation.\n    gt_boxes : List[datasets.training_sample.Box]\n      A list of datasets.training_sample.Box objects describing all ground\n      truth boxes in the image.\n    \"\"\"\n    # Merge in results for this single image\n    unsorted_predictions_by_class_index, object_count_by_class_index = self._compute_correctness_of_predictions(\n      scored_boxes_by_class_index = scored_boxes_by_class_index,\n      gt_boxes = gt_boxes) \n    for class_index, predictions in unsorted_predictions_by_class_index.items():\n      self._unsorted_predictions_by_class_index[class_index] += predictions\n    for class_index, count in object_count_by_class_index.items():\n      self._object_count_by_class_index[class_index] += object_count_by_class_index[class_index]\n\n  def _compute_average_precision(self, class_index):\n    # Sort predictions in descending order of score\n    sorted_predictions = sorted(self._unsorted_predictions_by_class_index[class_index], key = lambda prediction: prediction[0], reverse = True)\n    num_ground_truth_positives = self._object_count_by_class_index[class_index]\n\n    # Compute raw recall and precision arrays\n    recall_array = []\n    precision_array = []\n    true_positives = 0  # running tally\n    false_positives = 0 # \"\"\n    for i in range(len(sorted_predictions)):\n      true_positives += 1 if sorted_predictions[i][1] == True else 0\n      false_positives += 0 if sorted_predictions[i][1] == True else 1\n      recall = true_positives / num_ground_truth_positives\n      precision = true_positives / (true_positives + false_positives)\n      recall_array.append(recall)\n      precision_array.append(precision)\n\n    # Insert 0 at the beginning and end of the list. The 0 at the beginning won't\n    # matter due to how interpolation works, below.\n    recall_array.insert(0, 0.0)\n    recall_array.append(1.0)\n    precision_array.insert(0, 0.0)\n    precision_array.append(0.0)\n\n    # Interpolation means we compute the highest precision observed at a given\n    # recall value. Specifically, it means taking the maximum value seen from\n    # each point onward. See URL below:\n    # https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52#1a59\n    for i in range(len(precision_array)):\n      precision_array[i] = np.max(precision_array[i:])\n    \n    # Compute AP using simple rectangular integration under the curve\n    average_precision = 0\n    for i in range(len(recall_array) - 1):\n      dx = recall_array[i + 1] - recall_array[i + 0]\n      dy = precision_array[i + 1]\n      average_precision += dy * dx\n\n    return average_precision, recall_array, precision_array\n\n  def compute_mean_average_precision(self):\n    \"\"\"\n    Calculates mAP (mean average precision) using all the data accumulated thus\n    far. This should be called only after all image results have been\n    processed.\n\n    Returns\n    -------\n    np.float64\n      Mean average precision.\n    \"\"\"\n    average_precisions = []\n    for class_index in self._object_count_by_class_index:\n      average_precision, _, _ = self._compute_average_precision(class_index = class_index)\n      average_precisions.append(average_precision)\n    return np.mean(average_precisions)\n  \n  def plot_precision_vs_recall(self, class_index, class_name = None, interpolated = False):\n    \"\"\"\n    Plots precision (y axis) vs. recall (x axis) using all the data accumulated\n    thus far. This should be called only after all image results have been\n    processed.\n\n    Parameters\n    ----------\n    class_index : int\n      The class index for which the curve is plotted.\n    class_name : str\n      If given, used as the class name on the plot label. Otherwise, the\n      numeric class index is used directly.\n    \"\"\"\n    average_precision, recall_array, precision_array = self._compute_average_precision(class_index = class_index, interpolated = interpolated)\n\n    # Plot raw precision vs. recall\n    import matplotlib.pyplot as plt\n    label = \"{0} AP={1:1.2f}\".format(\"Class {}\".format(class_index) if class_name is None else class_name, average_precision)\n    plt.plot(recall_array, precision_array, label = label)\n    if interpolated:\n      plt.title(\"Precision (Interpolated) vs. Recall\")\n    else:\n      plt.title(\"Precision vs. Recall\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.legend()\n    plt.show()\n    plt.clf()\n\n  def plot_average_precisions(self, class_index_to_name): \n    # Compute average precisions for each class\n    labels = [ class_index_to_name[class_index] for class_index in self._object_count_by_class_index ]\n    average_precisions = []\n    for class_index in self._object_count_by_class_index:\n      average_precision, _, _ = self._compute_average_precision(class_index = class_index)\n      average_precisions.append(average_precision)\n\n    # Sort alphabetically by class name\n    sorted_results = sorted(zip(labels, average_precisions), reverse = True, key = lambda pair: pair[0])\n    labels, average_precisions = zip(*sorted_results) # unzip \n    \n    # Convert to %\n    average_precisions = np.array(average_precisions) * 100.0 # convert to %\n\n    # Bar plot\n    import matplotlib.pyplot as plt\n    plt.clf()\n    plt.xlim([0, 100])\n    plt.barh(labels, average_precisions)\n    plt.title(\"Model Performance\")\n    plt.xlabel(\"Average Precision (%)\")\n    for index, value in enumerate(average_precisions):\n      plt.text(value, index, \"%1.1f\" % value)\n    plt.show()\n\n  def print_average_precisions(self, class_index_to_name):\n    # Compute average precisions for each class\n    labels = [ class_index_to_name[class_index] for class_index in self._object_count_by_class_index ]\n    average_precisions = []\n    for class_index in self._object_count_by_class_index:\n      average_precision, _, _ = self._compute_average_precision(class_index = class_index)\n      average_precisions.append(average_precision)\n\n    # Sort by score (descending)\n    sorted_results = sorted(zip(labels, average_precisions), reverse = True, key = lambda pair: pair[1])\n    _, average_precisions = zip(*sorted_results) # unzip\n\n    # Maximum width of any class name (for pretty printing)\n    label_width = max([ len(label) for label in labels ])\n\n    # Pretty print\n    print(\"Average Precisions\")\n    print(\"------------------\")\n    for (label, average_precision) in sorted_results:\n      print(\"%s: %1.1f%%\" % (label.ljust(label_width), average_precision * 100.0))\n    print(\"------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:47.127719Z","iopub.execute_input":"2022-04-24T04:52:47.127952Z","iopub.status.idle":"2022-04-24T04:52:47.169092Z","shell.execute_reply.started":"2022-04-24T04:52:47.127920Z","shell.execute_reply":"2022-04-24T04:52:47.168378Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# visualize\n\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\n\ndef _draw_rectangle(ctx, corners, color, thickness = 4):\n  y_min, x_min, y_max, x_max = corners\n  ctx.rectangle(xy = [(x_min, y_min), (x_max, y_max)], outline = color, width = thickness)\n\ndef _draw_text(image, text, position, color, scale = 1.0, offset_lines = 0):\n  \"\"\"\n  Parameters\n  ----------\n  image : PIL.Image\n    Image object to draw on.\n  text : str\n    Text to render.\n  position : Tuple[float, float]\n    Location of top-left corner of text string in pixels.\n  offset_lines : float\n    Number of lines to offset the vertical position by, where a line is the\n    text height.\n  \"\"\"\n  font = ImageFont.load_default()\n  text_size = font.getsize(text)\n  text_image = Image.new(mode = \"RGBA\", size = text_size, color = (0, 0, 0, 0))\n  ctx = ImageDraw.Draw(text_image)\n  ctx.text(xy = (0, 0), text = text, font = font, fill = color)\n  scaled = text_image.resize((round(text_image.width * scale), round(text_image.height * scale)))\n  position = (round(position[0]), round(position[1] + offset_lines * scaled.height))\n  image.paste(im = scaled, box = position, mask = scaled)\n\ndef _class_to_color(class_index):\n  return list(ImageColor.colormap.values())[class_index + 1]\n\ndef show_anchors(output_path, image, anchor_map, anchor_valid_map, gt_rpn_map, gt_boxes, display = False):\n  ctx = ImageDraw.Draw(image, mode = \"RGBA\")\n  \n  # Draw all ground truth boxes with thick green lines\n  for box in gt_boxes:\n    _draw_rectangle(ctx, corners = box.corners, color = (0, 255, 0))\n\n  # Draw all object anchor boxes in yellow\n  for y in range(anchor_valid_map.shape[0]):\n    for x in range(anchor_valid_map.shape[1]):\n      for k in range(anchor_valid_map.shape[2]):  \n        if anchor_valid_map[y,x,k] <= 0 or gt_rpn_map[y,x,k,0] <= 0:\n          continue  # skip anchors excluded from training\n        if gt_rpn_map[y,x,k,1] < 1:\n          continue  # skip background anchors\n        height = anchor_map[y,x,k*4+2]\n        width = anchor_map[y,x,k*4+3]\n        cy = anchor_map[y,x,k*4+0]\n        cx = anchor_map[y,x,k*4+1]\n        corners = (cy - 0.5 * height, cx - 0.5 * width, cy + 0.5 * height, cx + 0.5 * width)\n        _draw_rectangle(ctx, corners = corners, color = (255, 255, 0), thickness = 3)\n \n  image.save(output_path)\n  if display:\n    image.show()\n\ndef show_detections(output_path, show_image, image, scored_boxes_by_class_index, class_index_to_name):\n  # Draw all results\n  ctx = ImageDraw.Draw(image, mode = \"RGBA\")\n  color_idx = 0\n  for class_index, scored_boxes in scored_boxes_by_class_index.items():\n    for i in range(scored_boxes.shape[0]):\n      scored_box = scored_boxes[i,:]\n      class_name = class_index_to_name[class_index]\n      text = \"%s %1.2f\" % (class_name, scored_box[4])\n      color = _class_to_color(class_index = class_index)\n      _draw_rectangle(ctx = ctx, corners = scored_box[0:4], color = color, thickness = 2)\n      _draw_text(image = image, text = text, position = (scored_box[1], scored_box[0]), color = color, scale = 1.5, offset_lines = -1)\n\n  # Output\n  if show_image:\n    image.show()\n  if output_path is not None:\n    image.save(output_path)\n    print(\"Wrote detection results to '%s'\" % output_path)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:47.170424Z","iopub.execute_input":"2022-04-24T04:52:47.170845Z","iopub.status.idle":"2022-04-24T04:52:47.216910Z","shell.execute_reply.started":"2022-04-24T04:52:47.170810Z","shell.execute_reply":"2022-04-24T04:52:47.216245Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# models/anchors\n\nimport itertools\nfrom math import sqrt\nimport numpy as np\n\n\ndef _compute_anchor_sizes():\n  #\n  # Anchor scales and aspect ratios.\n  #\n  # x * y = area          x * (x_aspect * x) = x_aspect * x^2 = area\n  # x_aspect * x = y  ->  x = sqrt(area / x_aspect)\n  #                       y = x_aspect * sqrt(area / x_aspect)\n  #\n  areas = [ 128*128, 256*256, 512*512 ]   # pixels\n  x_aspects = [ 0.5, 1.0, 2.0 ]           # x:1 ratio\n\n  # Generate all 9 combinations of area and aspect ratio\n  heights = np.array([ x_aspects[j] * sqrt(areas[i] / x_aspects[j]) for (i, j) in itertools.product(range(3), range(3)) ])\n  widths = np.array([ sqrt(areas[i] / x_aspects[j]) for (i, j) in itertools.product(range(3), range(3)) ])\n\n  # Return as (9,2) matrix of sizes\n  return np.vstack([ heights, widths ]).T\n\ndef generate_anchor_maps(image_shape, feature_pixels): \n  \"\"\"\n  Generates maps defining the anchors for a given input image size. There are 9\n  different anchors at each feature map cell (3 scales, 3 ratios).\n\n  Parameters\n  ----------\n  image_shape : Tuple[int, int, int]\n    Shape of the input image, (height, width, channels), at the scale it will\n    be passed into the Faster R-CNN model.\n  feature_pixels : int\n    Distance in pixels between anchors. This is the size, in input image space,\n    of each cell of the feature map output by the feature extractor stage of\n    the Faster R-CNN network.\n\n  Returns\n  -------\n  np.ndarray, np.ndarray\n    Two maps, with height and width corresponding to the feature map\n    dimensions, not the input image:\n      1. A map of shape (height, width, num_anchors*4) containing all anchors,\n         each stored as (center_y, center_x, anchor_height, anchor_width) in\n         input image pixel space.\n      2. A map of shape (height, width, num_anchors) indicating which anchors\n         are valid (1) or invalid (0). Invalid anchors are those that cross\n         image boundaries and must not be used during training.\n  \"\"\"\n\n  assert len(image_shape) == 3\n\n  #\n  # Note that precision can strongly affect anchor labeling in some images.\n  # Conversion of both operands to float32 matches the implementation by Yun\n  # Chen. That is, changing the final line so as to eliminate the conversion to\n  # float32:\n  #\n  #   return anchor_map, anchor_valid_map\n  #\n  # Has a pronounced effect on positive anchors in image 2008_000028.jpg in\n  # VOC2012.\n  #\n  \n  # Base anchor template: (num_anchors,4), with each anchor being specified by\n  # its corners (y1,x1,y2,x2)\n  anchor_sizes = _compute_anchor_sizes()\n  num_anchors = anchor_sizes.shape[0]\n  anchor_template = np.empty((num_anchors, 4))\n  anchor_template[:,0:2] = -0.5 * anchor_sizes  # y1, x1 (top-left)\n  anchor_template[:,2:4] = +0.5 * anchor_sizes  # y2, x2 (bottom-right)\n\n  # Shape of map, (H,W), determined by VGG-16 backbone\n  height, width = image_shape[0] // feature_pixels, image_shape[1] // feature_pixels\n\n  # Generate (H,W,2) map of coordinates, in feature space, each being [y,x]\n  y_cell_coords = np.arange(height)\n  x_cell_coords = np.arange(width)\n  cell_coords = np.array(np.meshgrid(y_cell_coords, x_cell_coords)).transpose([2, 1, 0])\n\n  # Convert all coordinates to image space (pixels) at *center* of each cell\n  center_points = cell_coords * feature_pixels + 0.5 * feature_pixels\n\n  # (H,W,2) -> (H,W,4), repeating the last dimension so it contains (y,x,y,x)\n  center_points = np.tile(center_points, reps = 2)\n\n  # (H,W,4) -> (H,W,4*num_anchors)\n  center_points = np.tile(center_points, reps = num_anchors)\n  \n  #\n  # Now we can create the anchors by adding the anchor template to each cell\n  # location. Anchor template is flattened to size num_anchors * 4 to make \n  # the addition possible (along the last dimension). \n  #\n  anchors = center_points.astype(np.float32) + anchor_template.flatten()\n\n  # (H,W,4*num_anchors) -> (H*W*num_anchors,4)\n  anchors = anchors.reshape((height*width*num_anchors, 4))\n\n  # Valid anchors are those that do not cross image boundaries\n  image_height, image_width = image_shape[0:2]\n  valid = np.all((anchors[:,0:2] >= [0,0]) & (anchors[:,2:4] <= [image_height,image_width]), axis = 1)\n\n  # Convert anchors to anchor format: (center_y, center_x, height, width)\n  anchor_map = np.empty((anchors.shape[0], 4))\n  anchor_map[:,0:2] = 0.5 * (anchors[:,0:2] + anchors[:,2:4])\n  anchor_map[:,2:4] = anchors[:,2:4] - anchors[:,0:2]\n\n  # Reshape maps and return\n  anchor_map = anchor_map.reshape((height, width, num_anchors * 4))\n  anchor_valid_map = valid.reshape((height, width, num_anchors))\n  return anchor_map.astype(np.float32), anchor_valid_map.astype(np.float32)\n\ndef generate_rpn_map(anchor_map, anchor_valid_map, gt_boxes, object_iou_threshold = 0.7, background_iou_threshold = 0.3):\n  \"\"\"\n  Generates a map containing ground truth data for training the region proposal\n  network.\n\n  Parameters\n  ----------\n  anchor_map : np.ndarray\n    Map of shape (height, width, num_anchors*4) defining the anchors as\n    (center_y, center_x, anchor_height, anchor_width) in input image space.\n  anchor_valid_map : np.ndarray\n    Map of shape (height, width, num_anchors) defining anchors that are valid\n    and may be included in training.\n  gt_boxes : List[training_sample.Box]\n    List of ground truth boxes.\n  object_iou_threshold : float\n    IoU threshold between an anchor and a ground truth box above which an\n    anchor is labeled as an object (positive) anchor.\n  background_iou_threshold : float\n    IoU threshold below which an anchor is labeled as background (negative).\n\n  Returns\n  -------\n  np.ndarray, np.ndarray, np.ndarray\n    RPN ground truth map, object (positive) anchor indices, and background\n    (negative) anchor indices. Map height and width dimensions are in feature\n    space.\n    1. RPN ground truth map of shape (height, width, num_anchors, 6) where the\n       last dimension is:\n       - 0: Trainable anchor (1) or not (0). Only valid and non-neutral (that\n            is, definitely positive or negative) anchors are trainable. This is\n            the same as anchor_valid_map with additional invalid anchors caused\n            by neutral samples\n       - 1: For trainable anchors, whether the anchor is an object anchor (1)\n            or background anchor (0). For non-trainable anchors, will be 0.\n       - 2: Regression target for box center, ty.\n       - 3: Regression target for box center, tx.\n       - 4: Regression target for box size, th.\n       - 5: Regression target for box size, tw.\n    2. Map of shape (N, 3) of indices (y, x, k) of all N object anchors in the\n       RPN ground truth map.\n    3. Map of shape (M, 3) of indices of all M background anchors in the RPN\n       ground truth map.\n  \"\"\"\n  height, width, num_anchors = anchor_valid_map.shape\n\n  # Convert ground truth box corners to (M,4) tensor and class indices to (M,)\n  gt_box_corners = np.array([ box.corners for box in gt_boxes ])\n  num_gt_boxes = len(gt_boxes)\n\n  # Compute ground truth box center points and side lengths\n  gt_box_centers = 0.5 * (gt_box_corners[:,0:2] + gt_box_corners[:,2:4])\n  gt_box_sides = gt_box_corners[:,2:4] - gt_box_corners[:,0:2]\n\n  # Flatten anchor boxes to (N,4) and convert to corners\n  anchor_map = anchor_map.reshape((-1,4))\n  anchors = np.empty(anchor_map.shape)\n  anchors[:,0:2] = anchor_map[:,0:2] - 0.5 * anchor_map[:,2:4]  # y1, x1\n  anchors[:,2:4] = anchor_map[:,0:2] + 0.5 * anchor_map[:,2:4]  # y2, x2\n  n = anchors.shape[0]\n\n  # Initialize all anchors initially as negative (background). We will also\n  # track which ground truth box was assigned to each anchor.\n  objectness_score = np.full(n, -1)   # RPN class: 0 = background, 1 = foreground, -1 = ignore (these will be marked as invalid in the truth map)\n  gt_box_assignments = np.full(n, -1) # -1 means no box\n  \n  # Compute IoU between each anchor and each ground truth box, (N,M).\n  ious = intersection_over_union(boxes1 = anchors, boxes2 = gt_box_corners)\n\n  # Need to remove anchors that are invalid (straddle image boundaries) from\n  # consideration entirely and the easiest way to do this is to wipe out their\n  # IoU scores\n  ious[anchor_valid_map.flatten() == 0, :] = -1.0\n\n  # Find the best IoU ground truth box for each anchor and the best IoU anchor\n  # for each ground truth box.\n  #\n  # Note that ious == max_iou_per_gt_box tests each of the N rows of ious\n  # against the M elements of max_iou_per_gt_box, column-wise. np.where() then\n  # returns all (y,x) indices of matches as a tuple: (y_indices, x_indices).\n  # The y indices correspond to the N dimension and therefore indicate anchors\n  # and the x indices correspond to the M dimension (ground truth boxes).\n  max_iou_per_anchor = np.max(ious, axis = 1)           # (N,)\n  best_box_idx_per_anchor = np.argmax(ious, axis = 1)   # (N,)\n  max_iou_per_gt_box = np.max(ious, axis = 0)           # (M,)\n  highest_iou_anchor_idxs = np.where(ious == max_iou_per_gt_box)[0] # get (L,) indices of anchors that are the highest-overlapping anchors for at least one of the M boxes\n\n  # Anchors below the minimum threshold are negative\n  objectness_score[max_iou_per_anchor < background_iou_threshold] = 0\n\n  # Anchors that meet the threshold IoU are positive\n  objectness_score[max_iou_per_anchor >= object_iou_threshold] = 1\n\n  # Anchors that overlap the most with ground truth boxes are positive\n  objectness_score[highest_iou_anchor_idxs] = 1\n\n  # We assign the highest IoU ground truth box to each anchor. If no box met\n  # the IoU threshold, the highest IoU box may happen to be a box for which\n  # the anchor had the highest IoU. If not, then the objectness score will be\n  # negative and the box regression won't ever be used.\n  gt_box_assignments[:] = best_box_idx_per_anchor\n\n  # Anchors that are to be ignored will be marked invalid. Generate a mask to\n  # multiply anchor_valid_map by (-1 -> 0, 0 or 1 -> 1). Then mark ignored\n  # anchors as 0 in objectness score because the score can only really be 0 or\n  # 1.\n  enable_mask = (objectness_score >= 0).astype(np.float32)\n  objectness_score[objectness_score < 0] = 0\n  \n  # Compute box delta regression targets for each anchor\n  box_delta_targets = np.empty((n, 4))\n  box_delta_targets[:,0:2] = (gt_box_centers[gt_box_assignments] - anchor_map[:,0:2]) / anchor_map[:,2:4] # ty = (box_center_y - anchor_center_y) / anchor_height, tx = (box_center_x - anchor_center_x) / anchor_width\n  box_delta_targets[:,2:4] = np.log(gt_box_sides[gt_box_assignments] / anchor_map[:,2:4])                 # th = log(box_height / anchor_height), tw = log(box_width / anchor_width)\n\n  # Assemble RPN ground truth map\n  rpn_map = np.zeros((height, width, num_anchors, 6))\n  rpn_map[:,:,:,0] = anchor_valid_map * enable_mask.reshape((height,width,num_anchors))  # trainable anchors (object or background; excludes boundary-crossing invalid and neutral anchors)\n  rpn_map[:,:,:,1] = objectness_score.reshape((height,width,num_anchors))\n  rpn_map[:,:,:,2:6] = box_delta_targets.reshape((height,width,num_anchors,4))\n  \n  # Return map along with positive and negative anchors\n  rpn_map_coords = np.transpose(np.mgrid[0:height,0:width,0:num_anchors], (1,2,3,0))                  # shape (height,width,k,3): every index (y,x,k,:) returns its own coordinate (y,x,k)\n  object_anchor_idxs = rpn_map_coords[np.where((rpn_map[:,:,:,1] > 0) & (rpn_map[:,:,:,0] > 0))]      # shape (N,3), where each row is the coordinate (y,x,k) of a positive sample\n  background_anchor_idxs = rpn_map_coords[np.where((rpn_map[:,:,:,1] == 0) & (rpn_map[:,:,:,0] > 0))] # shape (N,3), where each row is the coordinate (y,x,k) of a negative sample\n\n  return rpn_map.astype(np.float32), object_anchor_idxs, background_anchor_idxs\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:47.218241Z","iopub.execute_input":"2022-04-24T04:52:47.218483Z","iopub.status.idle":"2022-04-24T04:52:47.252190Z","shell.execute_reply.started":"2022-04-24T04:52:47.218443Z","shell.execute_reply":"2022-04-24T04:52:47.251439Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# models/roi_pooling_layer.py\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.layers import Layer\n\n\nclass RoIPoolingLayer(Layer):\n  \"\"\"\n  Input shape:\n    Two tensors [x_maps, x_rois] each with shape:\n      x_maps: (samples, height, width, channels), representing the feature maps for this batch, of type tf.float32\n      x_rois: (samples, num_rois, 4), where RoIs have the ordering (y, x, height, width), all tf.int32\n  Output shape:\n    (samples, num_rois, pool_size, pool_size, channels)\n  \"\"\"\n  def __init__(self, pool_size, **kwargs):\n    self.pool_size = pool_size\n    super().__init__(**kwargs)\n\n  def get_config(self):\n    config = {\n      \"pool_size\": self.pool_size,\n    }\n    base_config = super(RoIPoolingLayer, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  def compute_output_shape(self, input_shape):\n    map_shape, rois_shape = input_shape\n    assert len(map_shape) == 4 and len(rois_shape) == 3 and rois_shape[2] == 4\n    assert map_shape[0] == rois_shape[0]  # same number of samples\n    num_samples = map_shape[0]\n    num_channels = map_shape[3]\n    num_rois = rois_shape[1]\n    return (num_samples, num_rois, self.pool_size, self.pool_size, num_channels)\n\n  def call(self, inputs):\n    #\n    # Unused here but useful to know:\n    #\n    # When defining model, x_map.shape[0] will be None because we don't have a batch size.\n    # Using tf.shape() creates a dynamic scalar tensor that points to the batch size, and\n    # will be evaluated when it is known. See: https://github.com/tensorflow/tensorflow/issues/31991\n    #\n    #   x_map = inputs[0]\n    #   batch_size = tf.shape(x_map)[0]\n    #\n\n    #\n    # Inputs are a list, [ x_maps, x_rois ], where x_maps and x_rois must have\n    # the same batch size, N. The first application of map_fn() iterates over\n    # N samples of [ x_map, x_roi ]. For this to work, the data type of the\n    # final tensor must be specified otherwise map_fn() apparently infers a\n    # very different (and incorrect) input element.\n    #\n    # This basically iterates over every sample in the batch. This is the\n    # outer-most of a pair of map_fn() iterations and stacks its results\n    # in the batch dimension, (samples, ...).\n    #\n    if self.pool_size == 7 and inputs[0].shape[3] == 512:\n      # Special case optimization: 7x7x512 pools, ~4-5x speed-up\n      return tf.map_fn(\n        fn = lambda input_pair:\n          RoIPoolingLayer._compute_pooled_rois_7x7x512(feature_map = input_pair[0], rois = input_pair[1]),\n        elems = inputs,\n        fn_output_signature = tf.float32  # this is absolutely required else the fn type inference seems to fail spectacularly\n      )\n    else:\n      # Generic case capable of handling any pool shape\n      return tf.map_fn(\n        fn = lambda input_pair:\n          RoIPoolingLayer._compute_pooled_rois(feature_map = input_pair[0], rois = input_pair[1], pool_size = self.pool_size),\n        elems = inputs,\n        fn_output_signature = tf.float32  # this is absolutely required else the fn type inference seems to fail spectacularly\n      )\n\n  @tf.function\n  def _compute_pooled_rois(feature_map, rois, pool_size):\n    #\n    # Given a feature map and its associated RoIs, iterate over all RoIs for\n    # this map. This is the second level of iteration and yields the num_rois\n    # dimension: (samples, num_rois, ...)\n    #\n    return tf.map_fn(\n      fn = lambda roi:\n        RoIPoolingLayer._compute_pooled_roi(feature_map = feature_map, roi = roi, pool_size = pool_size),\n      elems = rois,\n      fn_output_signature = tf.float32\n    )\n\n  @tf.function\n  def _compute_pooled_roi(feature_map, roi, pool_size):\n    #\n    # Given a feature map and a single RoI, computes the pooled map of shape\n    # (pool_size, pool_size).\n    #\n\n    # Crop out the region of interest from the feature map\n    region_y = roi[0]\n    region_x = roi[1]\n    region_height = roi[2]\n    region_width = roi[3]\n    num_channels = feature_map.shape[2]\n    region_of_interest = tf.slice(feature_map, [region_y, region_x, 0], [region_height, region_width, num_channels])\n\n    # Compute step size within the region of interest (feature map)\n    x_step = tf.cast(region_width, dtype = tf.float32) / tf.cast(pool_size, dtype = tf.float32)\n    y_step = tf.cast(region_height, dtype = tf.float32) / tf.cast(pool_size, dtype = tf.float32)\n\n    #\n    # Compute the pooled map for this RoI having shape (pool_size, pool_size).\n    # This is done by a nested iteration with x being the inner, fast index and\n    # y being the outer, slow index, resulting in shape (size_y, size_x), where\n    # both sizes here are pool_size.\n    #\n    x_range = tf.cast(tf.range(pool_size), dtype = tf.float32)\n    y_range = tf.cast(tf.range(pool_size), dtype = tf.float32)\n    pooled_cells = tf.map_fn(\n      fn = lambda y: tf.map_fn(\n        fn = lambda x:\n          RoIPoolingLayer._pool_one_cell(region_of_interest, pool_y_start = y, pool_x_start = x, y_step = y_step, x_step = x_step, region_height = region_height, region_width = region_width, pool_size = pool_size, num_channels = num_channels),\n        elems = x_range\n      ),\n      elems = y_range\n    )\n    return pooled_cells\n\n  @tf.function\n  def _pool_one_cell(region_of_interest, pool_y_start, pool_x_start, y_step, x_step, region_height, region_width, pool_size, num_channels):\n    #\n    # This function maps a single pooling cell over some part of the RoI and\n    # then computes the max of the RoI cells inside that pooling cell. The\n    # operation is performed per-channel, yielding a result of shape\n    # (1, num_channels).\n    #\n    # Compute the start and end positions using the following logic:\n    #\n    #   x_start = int(x * x_step)\n    #   x_end = int((x + 1) * x_step) if (x + 1) < pool_size else region_width\n    #   y_start = int(y * y_step)\n    #   y_end = int((y + 1) * y_step) if (y + 1) < pool_size else region_height\n    #\n    pool_y_start_int = tf.cast(pool_y_start, dtype = tf.int32)\n    pool_x_start_int = tf.cast(pool_x_start, dtype = tf.int32)\n    y_start = tf.cast(pool_y_start * y_step, dtype = tf.int32)\n    x_start = tf.cast(pool_x_start * x_step, dtype = tf.int32)\n    y_end = tf.cond((pool_y_start_int + 1) < pool_size,\n      lambda: tf.cast((pool_y_start + 1) * y_step, dtype = tf.int32),\n      lambda: region_height\n    )\n    x_end = tf.cond((pool_x_start_int + 1) < pool_size,\n      lambda: tf.cast((pool_x_start + 1) * x_step, dtype = tf.int32),\n      lambda: region_width\n    )\n\n    # Extract this cell from the region and return the max\n    y_size = tf.math.maximum(y_end - y_start, 1)  # if RoI is smaller than pool area, y_end - y_start can be less than 1 (0); we want to sample at least one cell\n    x_size = tf.math.maximum(x_end - x_start, 1)\n    pool_cell = tf.slice(region_of_interest, [y_start, x_start, 0], [y_size, x_size, num_channels])\n    return tf.math.reduce_max(pool_cell, axis=(1,0))  # keep channels independent\n\n  @tf.function\n  def _compute_pooled_rois_7x7x512(feature_map, rois):\n    # Special case: 7x7x512, unrolled pool width and height (7x7=49)\n    return tf.map_fn(\n      fn = lambda roi: tf.reshape(\n        tf.stack([\n          # y=0,x=0\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=0,x=1\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=0,x=2\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=0,x=3\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=0,x=4\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=0,x=5\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=0,x=6\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, roi[3] - tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=1,x=0\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=1,x=1\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=1,x=2\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=1,x=3\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=1,x=4\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=1,x=5\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=1,x=6\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, roi[3] - tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=2,x=0\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=2,x=1\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=2,x=2\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=2,x=3\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=2,x=4\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=2,x=5\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=2,x=6\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, roi[3] - tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=3,x=0\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=3,x=1\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=3,x=2\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=3,x=3\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=3,x=4\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=3,x=5\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=3,x=6\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, roi[3] - tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=4,x=0\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=4,x=1\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=4,x=2\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=4,x=3\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=4,x=4\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=4,x=5\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=4,x=6\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, roi[3] - tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=5,x=0\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=5,x=1\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=5,x=2\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=5,x=3\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=5,x=4\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=5,x=5\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=5,x=6\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, roi[3] - tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=6,x=0\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, roi[2] - tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((0 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(0 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=6,x=1\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, roi[2] - tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((1 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(1 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=6,x=2\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, roi[2] - tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((2 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(2 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=6,x=3\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, roi[2] - tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((3 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(3 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=6,x=4\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, roi[2] - tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((4 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(4 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=6,x=5\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, roi[2] - tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, tf.cast((5 + 1) * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32) - tf.cast(5 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n          # y=6,x=6\n          tf.math.reduce_max(\n            tf.slice(\n              feature_map[ roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3], 0:512 ],\n              [\n                tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32),\n                tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32),\n                0\n              ],\n              [\n                tf.math.maximum(1, roi[2] - tf.cast(6 * (tf.cast(roi[2], dtype = tf.float32) / 7), dtype = tf.int32)),\n                tf.math.maximum(1, roi[3] - tf.cast(6 * (tf.cast(roi[3], dtype = tf.float32) / 7), dtype = tf.int32)),\n                512\n              ]\n            ),\n            axis = (1,0)\n          ),\n        ]),\n        shape = (7,7,512)\n      ),\n      elems = rois,\n      fn_output_signature = tf.float32\n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:47.254431Z","iopub.execute_input":"2022-04-24T04:52:47.254951Z","iopub.status.idle":"2022-04-24T04:52:52.758479Z","shell.execute_reply.started":"2022-04-24T04:52:47.254915Z","shell.execute_reply":"2022-04-24T04:52:52.757663Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# models/detector.py\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Lambda\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import backend as K\n\n\nclass DetectorNetwork(tf.keras.Model):\n  def __init__(self, num_classes, custom_roi_pool, activate_class_outputs, l2, dropout_probability):\n    super().__init__()\n\n    self._num_classes = num_classes\n    self._activate_class_outputs = activate_class_outputs\n    self._dropout_probability = dropout_probability\n\n    regularizer = tf.keras.regularizers.l2(l2)\n    class_initializer = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.01)\n    regressor_initializer = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.001)\n\n    # If custom_roi_pool flag is set, we use our custom implementation,\n    # otherwise, tf operations that can approximate the operation will be used\n    # in call().\n    self._roi_pool = RoIPoolingLayer(pool_size = 7, name = \"custom_roi_pool\") if custom_roi_pool else None\n\n    # Fully-connected layers with optional dropout. Initial weights will be\n    # loaded from pre-trained VGG-16 ImageNet model by parent Faster R-CNN\n    # module. These layers act as classifiers as in VGG-16 and use the same\n    # names as Keras' built-in implementation of VGG-16. TimeDistributed() is\n    # used to iterate over the proposal dimension and apply the layer to each\n    # of the proposals.\n    self._flatten = TimeDistributed(Flatten())\n    self._fc1 = TimeDistributed(name = \"fc1\", layer = Dense(units = 4096, activation = \"relu\", kernel_regularizer = regularizer))\n    self._dropout1 = TimeDistributed(Dropout(dropout_probability))\n    self._fc2 = TimeDistributed(name = \"fc2\", layer = Dense(units = 4096, activation = \"relu\", kernel_regularizer = regularizer))\n    self._dropout2 = TimeDistributed(Dropout(dropout_probability))\n\n    # Output: classifier\n    class_activation = \"softmax\" if activate_class_outputs else None\n    self._classifier = TimeDistributed(name = \"classifier_class\", layer = Dense(units = num_classes, activation = class_activation, kernel_initializer = class_initializer))\n\n    # Output: box delta regressions. Unique regression weights for each\n    # possible class excluding background class, hence the use of\n    # (num_classes-1). Class index 1 regressions are therefore at\n    # indices: 0*4:0*4+1.\n    self._regressor = TimeDistributed(name = \"classifier_boxes\", layer = Dense(units = 4 * (num_classes - 1), activation = \"linear\", kernel_initializer = regressor_initializer))\n\n  def call(self, inputs, training):\n    # Unpack inputs\n    input_image = inputs[0]\n    feature_map = inputs[1]\n    proposals = inputs[2]\n    assert len(feature_map.shape) == 4\n\n    # RoI pooling: creates a 7x7 map for each proposal (1, num_rois, 7, 7, 512)\n    if self._roi_pool:\n      # Use our custom layer. Need to convert proposals from image-space\n      # (y1, x1, y2, x2) to feature map space (y1, x1, height, width).\n      proposals = tf.cast(proposals, dtype = tf.int32)                  # RoIs must be integral for RoIPoolingLayer\n      map_dimensions = tf.shape(feature_map)[1:3]                       # (batches, height, width, channels) -> (height, width)\n      map_limits = tf.tile(map_dimensions, multiples = [2]) - 1         # (height, width, height, width)\n      roi_corners = tf.minimum(proposals // 16, map_limits)             # to feature map space and clamp against map edges\n      roi_corners = tf.maximum(roi_corners, 0)\n      roi_dimensions = roi_corners[:,2:4] - roi_corners[:,0:2] + 1\n      rois = tf.concat([ roi_corners[:,0:2], roi_dimensions ], axis = 1)  # (N,4), where each row is (y1, x2, height, width) in feature map units\n      rois = tf.expand_dims(rois, axis = 0)                             # (1,N,4), batch size of 1, as expected by RoIPoolingLayer\n      pool = RoIPoolingLayer(pool_size = 7, name = \"roi_pool\")([feature_map, rois])\n    else:\n      # Crop the proposals, resize to 14x14 (with bilinear interpolation) and\n      # max pool down to 7x7. This works just as well and is used in several\n      # TensorFlow implementations of Faster R-CNN, such as:\n      # https://github.com/kevinjliang/tf-Faster-RCNN/blob/master/Lib/roi_pool.py\n\n      # Convert to normalized RoIs with each coordinate in [0,1]\n      image_height = tf.shape(input_image)[1] # height in pixels\n      image_width = tf.shape(input_image)[2]  # width in pixels\n      rois = proposals / [ image_height, image_width, image_height, image_width ]\n\n      # Crop, resize, pool\n      num_rois = tf.shape(rois)[0];\n      region = tf.image.crop_and_resize(image = feature_map, boxes = rois, box_indices = tf.zeros(num_rois, dtype = tf.int32), crop_size = [14, 14])\n      pool = tf.nn.max_pool(region, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n      pool = tf.expand_dims(pool, axis = 0) # (num_rois, 7, 7, 512) -> (1, num_rois, 7, 7, 512)\n          \n    # Pass through final layers\n    flattened = self._flatten(pool)\n    if training and self._dropout_probability != 0:\n      fc1 = self._fc1(flattened)\n      do1 = self._dropout1(fc1)\n      fc2 = self._fc2(do1)\n      do2 = self._dropout2(fc2)\n      out = do2\n    else:\n      fc1 = self._fc1(flattened)\n      fc2 = self._fc2(fc1)\n      out = fc2 \n    class_activation = \"softmax\" if self._activate_class_outputs else None\n    classes = self._classifier(out)\n    box_deltas = self._regressor(out)\n\n    return [ classes, box_deltas ]\n\n  @staticmethod\n  def class_loss(y_predicted, y_true, from_logits):\n    \"\"\"\n    Computes detector network classification loss.\n\n    Parameters\n    ----------\n    y_predicted : tf.Tensor\n      Class predictions, shaped (1, N, num_classes), where N is the number of\n      detections (i.e., the number of proposals fed into the detector network).\n    y_true : tf.Tensor\n      Ground truth, shaped (1, N, num_classes). One-hot-encoded labels.\n    from_logits : bool\n      If true, y_predicted is given as logits (that is, softmax was not\n      applied), otherwise, as probability scores (softmax applied).\n\n    Returns\n    -------\n    tf.Tensor\n      Scalar loss.\n    \"\"\"\n    scale_factor = 1.0\n    N = tf.cast(tf.shape(y_true)[1], dtype = tf.float32) + K.epsilon()  # number of proposals\n    if from_logits:\n      return scale_factor * K.sum(K.categorical_crossentropy(target = y_true, output = y_predicted, from_logits = True)) / N\n    else:\n      return scale_factor * K.sum(K.categorical_crossentropy(y_true, y_predicted)) / N\n  \n  @staticmethod\n  def regression_loss(y_predicted, y_true):\n    \"\"\"\n    Computes detector network box delta regression loss.\n\n    Parameters\n    ----------\n    y_predicted : tf.Tensor\n      Predicted box delta regressions in parameterized form (ty, tx, th, tw).\n      Shaped (1, N, 4 * (num_classes - 1)). Class 0 (background) obviously has\n      no box associated with it.\n    y_true : tf.Tensor\n      Ground truth box delta regression targets, shaped\n      (1, N, 2, 4 * (num_classes - 1)). Elements [:,:,0,:] are masks indicating\n      which of the regression targets [:,:,1,:] to use for the given proposal.\n      That is, [0,n,0,:] is an array of 1 or 0 indicating which of [0,n,1,:]\n      are valid for inclusion in the loss. For non-background proposals, there\n      will be 4 unmasked values corresponding to (ty, tx, th, tw).\n    \n    Returns\n    -------\n    tf.Tensor\n      Scalar loss.\n    \"\"\"\n    scale_factor = 1.0\n    sigma = 1.0\n    sigma_squared = sigma * sigma\n  \n    # We want to unpack the regression targets and the mask of valid targets into\n    # tensors each of the same shape as the predicted: \n    #   (batch_size, num_proposals, 4*(num_classes-1))\n    # y_true has shape:\n    #   (batch_size, num_proposals, 2, 4*(num_classes-1))\n    y_mask = y_true[:,:,0,:]\n    y_true_targets = y_true[:,:,1,:]\n  \n    # Compute element-wise loss using robust L1 function for all 4 regression\n    # targets\n    x = y_true_targets - y_predicted\n    x_abs = tf.math.abs(x)\n    is_negative_branch = tf.stop_gradient(tf.cast(tf.less(x_abs, 1.0 / sigma_squared), dtype = tf.float32))\n    R_negative_branch = 0.5 * x * x * sigma_squared\n    R_positive_branch = x_abs - 0.5 / sigma_squared\n    losses = is_negative_branch * R_negative_branch + (1.0 - is_negative_branch) * R_positive_branch\n  \n    # Accumulate the relevant terms and normalize by the number of proposals\n    N = tf.cast(tf.shape(y_true)[1], dtype = tf.float32) + K.epsilon()  # N = number of proposals\n    relevant_loss_terms = y_mask * losses\n    return scale_factor * K.sum(relevant_loss_terms) / N\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:52.759753Z","iopub.execute_input":"2022-04-24T04:52:52.760011Z","iopub.status.idle":"2022-04-24T04:52:52.792822Z","shell.execute_reply.started":"2022-04-24T04:52:52.759967Z","shell.execute_reply":"2022-04-24T04:52:52.792195Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# models/rpn.py\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras import backend as K\n\n\nclass RegionProposalNetwork(tf.keras.Model):\n  def __init__(self, max_proposals_pre_nms_train, max_proposals_post_nms_train, max_proposals_pre_nms_infer, max_proposals_post_nms_infer, l2 = 0, allow_edge_proposals = False):\n    super().__init__()\n\n    self._max_proposals_pre_nms_train = max_proposals_pre_nms_train\n    self._max_proposals_post_nms_train = max_proposals_post_nms_train\n    self._max_proposals_pre_nms_infer = max_proposals_pre_nms_infer\n    self._max_proposals_post_nms_infer = max_proposals_post_nms_infer\n    self._allow_edge_proposals = allow_edge_proposals\n\n    regularizer = tf.keras.regularizers.l2(l2)\n    initial_weights = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.01, seed = None)\n\n    anchors_per_location = 9\n\n    # 3x3 convolution over input map producing 512-d result at each output. The center of each output is an anchor point (k anchors at each point).\n    self._rpn_conv1 = Conv2D(name = \"rpn_conv1\", kernel_size = (3,3), strides = 1, filters = 512, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n\n    # Classification layer: predicts whether there is an object at the anchor or not. We use a sigmoid function, where > 0.5 is indicates a positive result.\n    self._rpn_class = Conv2D(name = \"rpn_class\", kernel_size = (1,1), strides = 1, filters = anchors_per_location, padding = \"same\", activation = \"sigmoid\", kernel_initializer = initial_weights)\n\n    # Box delta regression\n    self._rpn_boxes = Conv2D(name = \"rpn_boxes\", kernel_size = (1,1), strides = 1, filters = 4 * anchors_per_location, padding = \"same\", activation = None, kernel_initializer = initial_weights)\n\n  def __call__(self, inputs, training):\n    # Unpack inputs\n    input_image = inputs[0]\n    feature_map = inputs[1]\n    anchor_map = inputs[2]\n    anchor_valid_map = inputs[3]\n    assert len(feature_map.shape) == 4\n\n    # Proposal sampling depends on whether we are training or not\n    if training:\n      max_proposals_pre_nms = self._max_proposals_pre_nms_train\n      max_proposals_post_nms = self._max_proposals_post_nms_train\n    else:\n      max_proposals_pre_nms = self._max_proposals_pre_nms_infer\n      max_proposals_post_nms = self._max_proposals_post_nms_infer\n\n    # Pass through network\n    y = self._rpn_conv1(feature_map)\n    scores = self._rpn_class(y)\n    box_delta_regressions = self._rpn_boxes(y)\n\n    # Extract valid\n    anchors, objectness_scores, box_deltas = self._extract_valid(\n      anchor_map = anchor_map,\n      anchor_valid_map = anchor_valid_map,\n      objectness_score_map = scores,\n      box_delta_map = box_delta_regressions,\n      allow_edge_proposals = self._allow_edge_proposals\n    )\n\n    # Convert regressions to box corners\n    proposals = tf_convert_deltas_to_boxes(\n      box_deltas = box_deltas,\n      anchors = anchors,\n      box_delta_means = [ 0.0, 0.0, 0.0, 0.0 ],\n      box_delta_stds = [ 1.0, 1.0, 1.0, 1.0 ]\n    )\n    \n    # Keep only the top-N scores. Note that we do not care whether the\n    # proposals were labeled as objects (score > 0.5) and peform a simple\n    # ranking among all of them. Restricting them has a strong adverse impact\n    # on training performance.\n    sorted_indices = tf.argsort(objectness_scores)                  # sort in ascending order of objectness score\n    sorted_indices = sorted_indices[::-1]                           # descending order of score\n    proposals = tf.gather(proposals, indices = sorted_indices)[0:max_proposals_pre_nms] # grab the top-N best proposals\n    objectness_scores = tf.gather(objectness_scores, indices = sorted_indices)[0:max_proposals_pre_nms] # corresponding scores\n\n    # Clip to image boundaries\n    image_height = tf.cast(tf.shape(input_image)[1], dtype = tf.float32)  # height in pixels (shape of image is (batches,height,width,channels))\n    image_width = tf.cast(tf.shape(input_image)[2], dtype = tf.float32)   # width in pixels\n    proposals_top_left = tf.maximum(proposals[:,0:2], 0.0)\n    proposals_y2 = tf.reshape(tf.minimum(proposals[:,2], image_height), shape = (-1, 1))  # slice operation produces [N,], reshape to [N,1]\n    proposals_x2 = tf.reshape(tf.minimum(proposals[:,3], image_width), shape = (-1, 1))\n    proposals = tf.concat([ proposals_top_left, proposals_y2, proposals_x2 ], axis = 1) # [N,4] proposal tensor\n\n    # Remove anything less than 16 pixels on a side\n    height = proposals[:,2] - proposals[:,0]\n    width = proposals[:,3] - proposals[:,1]\n    idxs = tf.where((height >= 16) & (width >= 16))\n    proposals = tf.gather_nd(proposals, indices = idxs)\n    objectness_scores = tf.gather_nd(objectness_scores, indices = idxs)\n\n    # Perform NMS\n    idxs = tf.image.non_max_suppression(\n      boxes = proposals,\n      scores = objectness_scores,\n      max_output_size = max_proposals_post_nms,\n      iou_threshold = 0.7\n    )\n    proposals = tf.gather(proposals, indices = idxs)\n\n    return [ scores, box_delta_regressions, proposals ] \n\n  def _extract_valid(self, anchor_map, anchor_valid_map, objectness_score_map, box_delta_map, allow_edge_proposals):\n    # anchor_valid_map shape is (batch,height,width,num_anchors)\n    height = tf.shape(anchor_valid_map)[1]\n    width = tf.shape(anchor_valid_map)[2]\n    num_anchors = tf.shape(anchor_valid_map)[3]\n  \n    anchors = tf.reshape(anchor_map, shape = (height * width * num_anchors, 4))             # [N,4], all anchors \n    anchors_valid = tf.reshape(anchor_valid_map, shape = (height * width * num_anchors, 1)) # [N,1], whether anchors are valid (i.e., do not cross image boundaries)\n    scores = tf.reshape(objectness_score_map, shape = (height * width * num_anchors, 1))    # [N,1], predicted objectness scores\n    box_deltas = tf.reshape(box_delta_map, shape = (height * width * num_anchors, 4))       # [N,4], predicted box delta regression targets\n    \n    anchors_valid = tf.squeeze(anchors_valid)                                               # [N,]\n    scores = tf.squeeze(scores)                                                             # [N,]\n  \n    if allow_edge_proposals:\n      # Use all proposals\n      return anchors, scores, box_deltas\n    else:\n      # Filter out those proposals generated at invalid anchors. Invalid\n      # anchors are really just those that cross image boundaries and, counter-\n      # intuitively, given that the Faster R-CNN paper (Section 3.3) says that\n      # these anchors are ignored during loss calculation, they should in fact\n      # be included when generating proposals. Good performance requires\n      # evaluating lots of proposals, so even if cross-boundary anchors do not\n      # contribute to RPN loss, they can still feed samples into the detector\n      # stage. It is therefore not recommended to exclude edge proposals but\n      # the option exists here for educational purposes.\n      idxs = tf.where(anchors_valid > 0)\n      return tf.gather_nd(anchors, indices = idxs), tf.gather_nd(scores, indices = idxs), tf.gather_nd(box_deltas, indices = idxs)\n\n  @staticmethod\n  def class_loss(y_predicted, gt_rpn_map):\n    \"\"\"\n    Computes RPN class loss.\n  \n    Parameters\n    ----------\n    y_predicted : tf.Tensor\n      A tensor of shape (batch_size, height, width, num_anchors) containing\n      objectness scores (0 = background, 1 = object).\n    gt_rpn_map : tf.Tensor\n      Ground truth tensor of shape (batch_size, height, width, num_anchors, 6).\n  \n    Returns\n    -------\n    tf.Tensor\n      Scalar loss.\n    \"\"\"\n  \n    # y_true_class: (batch_size, height, width, num_anchors), same as predicted_scores\n    y_true_class = tf.reshape(gt_rpn_map[:,:,:,:,1], shape = tf.shape(y_predicted))\n    y_predicted_class = y_predicted\n    \n    # y_mask: y_true[:,:,:,0] is 1.0 for anchors included in the mini-batch\n    y_mask = tf.reshape(gt_rpn_map[:,:,:,:,0], shape = tf.shape(y_predicted_class))\n  \n    # Compute how many anchors are actually used in the mini-batch (e.g.,\n    # typically 256)\n    N_cls = tf.cast(tf.math.count_nonzero(y_mask), dtype = tf.float32) + K.epsilon()\n  \n    # Compute element-wise loss for all anchors\n    loss_all_anchors = K.binary_crossentropy(y_true_class, y_predicted_class)\n    \n    # Zero out the ones which should not have been included\n    relevant_loss_terms = y_mask * loss_all_anchors\n  \n    # Sum the total loss and normalize by the number of anchors used\n    return K.sum(relevant_loss_terms) / N_cls\n  \n  @staticmethod\n  def regression_loss(y_predicted, gt_rpn_map):\n    \"\"\"\n    Computes RPN box delta regression loss.\n  \n    Parameters\n    ----------\n    y_predicted : tf.Tensor\n      A tensor of shape (batch_size, height, width, num_anchors * 4) containing\n      RoI box delta regressions for each anchor, stored as: ty, tx, th, tw.\n    gt_rpn_map : tf.Tensor\n      Ground truth tensor of shape (batch_size, height, width, num_anchors, 6).\n  \n    Returns\n    -------\n    tf.Tensor\n      Scalar loss.\n    \"\"\"\n  \n    scale_factor = 1.0  # hyper-parameter that controls magnitude of regression loss and is chosen to make regression term comparable to class term\n    sigma = 3.0         # see: https://github.com/rbgirshick/py-faster-rcnn/issues/89\n    sigma_squared = sigma * sigma\n  \n    y_predicted_regression = y_predicted\n    y_true_regression = tf.reshape(gt_rpn_map[:,:,:,:,2:6], shape = tf.shape(y_predicted_regression))\n  \n    # Include only anchors that are used in the mini-batch and which correspond\n    # to objects (positive samples)\n    y_included = tf.reshape(gt_rpn_map[:,:,:,:,0], shape = tf.shape(gt_rpn_map)[0:4]) # trainable anchors map: (batch_size, height, width, num_anchors)\n    y_positive = tf.reshape(gt_rpn_map[:,:,:,:,1], shape = tf.shape(gt_rpn_map)[0:4]) # positive anchors\n    y_mask = y_included * y_positive\n  \n    # y_mask is of the wrong shape. We have one value per (y,x,k) position but in\n    # fact need to have 4 values (one for each of the regression variables). For\n    # example, y_predicted might be (1,37,50,36) and y_mask will be (1,37,50,9).\n    # We need to repeat the last dimension 4 times.\n    y_mask = tf.repeat(y_mask, repeats = 4, axis = 3)\n  \n    # The paper normalizes by dividing by a quantity called N_reg, which is equal\n    # to the total number of anchors (~2400) and then multiplying by lambda=10.\n    # This does not make sense to me because we are summing over a mini-batch at\n    # most, so we use N_cls here. I might be misunderstanding what is going on\n    # but 10/2400 = 1/240 which is pretty close to 1/256 and the paper mentions\n    # that training is relatively insensitve to choice of normalization.\n    N_cls = tf.cast(tf.math.count_nonzero(y_included), dtype = tf.float32) + K.epsilon()\n  \n    # Compute element-wise loss using robust L1 function for all 4 regression\n    # components\n    x = y_true_regression - y_predicted_regression\n    x_abs = tf.math.abs(x)\n    is_negative_branch = tf.stop_gradient(tf.cast(tf.less(x_abs, 1.0 / sigma_squared), dtype = tf.float32))\n    R_negative_branch = 0.5 * x * x * sigma_squared\n    R_positive_branch = x_abs - 0.5 / sigma_squared\n    loss_all_anchors = is_negative_branch * R_negative_branch + (1.0 - is_negative_branch) * R_positive_branch\n  \n    # Zero out the ones which should not have been included\n    relevant_loss_terms = y_mask * loss_all_anchors\n    return scale_factor * K.sum(relevant_loss_terms) / N_cls\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:52.794236Z","iopub.execute_input":"2022-04-24T04:52:52.794661Z","iopub.status.idle":"2022-04-24T04:52:52.836721Z","shell.execute_reply.started":"2022-04-24T04:52:52.794609Z","shell.execute_reply":"2022-04-24T04:52:52.835891Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# models/vgg16.py\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.initializers import glorot_normal\n\n\nclass FeatureExtractor(tf.keras.Model):\n  def __init__(self, l2 = 0):\n    super().__init__()\n\n    initial_weights = glorot_normal()\n    regularizer = tf.keras.regularizers.l2(l2)\n    input_shape = (None, None, 3)\n  \n    # First two convolutional blocks are frozen (not trainable)\n    self._block1_conv1 = Conv2D(name = \"block1_conv1\", input_shape = input_shape, kernel_size = (3,3), strides = 1, filters = 64, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, trainable = False)\n    self._block1_conv2 = Conv2D(name = \"block1_conv2\", kernel_size = (3,3), strides = 1, filters = 64, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, trainable = False)\n    self._block1_maxpool = MaxPooling2D(pool_size = 2, strides = 2)\n\n    self._block2_conv1 = Conv2D(name = \"block2_conv1\", kernel_size = (3,3), strides = 1, filters = 128, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, trainable = False)\n    self._block2_conv2 = Conv2D(name = \"block2_conv2\", kernel_size = (3,3), strides = 1, filters = 128, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, trainable = False)\n    self._block2_maxpool = MaxPooling2D(pool_size = 2, strides = 2)\n\n    # Weight decay begins from these layers onward: https://github.com/rbgirshick/py-faster-rcnn/blob/master/models/pascal_voc/VGG16/faster_rcnn_end2end/train.prototxt\n    self._block3_conv1 = Conv2D(name = \"block3_conv1\", kernel_size = (3,3), strides = 1, filters = 256, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block3_conv2 = Conv2D(name = \"block3_conv2\", kernel_size = (3,3), strides = 1, filters = 256, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block3_conv3 = Conv2D(name = \"block3_conv3\", kernel_size = (3,3), strides = 1, filters = 256, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block3_maxpool = MaxPooling2D(pool_size = 2, strides = 2)\n\n    self._block4_conv1 = Conv2D(name = \"block4_conv1\", kernel_size = (3,3), strides = 1, filters = 512, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block4_conv2 = Conv2D(name = \"block4_conv2\", kernel_size = (3,3), strides = 1, filters = 512, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block4_conv3 = Conv2D(name = \"block4_conv3\", kernel_size = (3,3), strides = 1, filters = 512, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block4_maxpool = MaxPooling2D(pool_size = 2, strides = 2)\n\n    self._block5_conv1 = Conv2D(name = \"block5_conv1\", kernel_size = (3,3), strides = 1, filters = 512, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block5_conv2 = Conv2D(name = \"block5_conv2\", kernel_size = (3,3), strides = 1, filters = 512, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n    self._block5_conv3 = Conv2D(name = \"block5_conv3\", kernel_size = (3,3), strides = 1, filters = 512, padding = \"same\", activation = \"relu\", kernel_initializer = initial_weights, kernel_regularizer = regularizer)\n\n  def call(self, input_image):\n    y = self._block1_conv1(input_image)\n    y = self._block1_conv2(y)\n    y = self._block1_maxpool(y)\n\n    y = self._block2_conv1(y)\n    y = self._block2_conv2(y)\n    y = self._block2_maxpool(y)\n\n    y = self._block3_conv1(y)\n    y = self._block3_conv2(y)\n    y = self._block3_conv3(y)\n    y = self._block3_maxpool(y)\n\n    y = self._block4_conv1(y)\n    y = self._block4_conv2(y)\n    y = self._block4_conv3(y)\n    y = self._block4_maxpool(y)\n\n    y = self._block5_conv1(y)\n    y = self._block5_conv2(y)\n    y = self._block5_conv3(y)\n\n    return y\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:52.838685Z","iopub.execute_input":"2022-04-24T04:52:52.839337Z","iopub.status.idle":"2022-04-24T04:52:52.865194Z","shell.execute_reply.started":"2022-04-24T04:52:52.839295Z","shell.execute_reply":"2022-04-24T04:52:52.864324Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# models/faster_rcnn.py\n#\n# TensorFlow/Keras implementation of Faster R-CNN training and inference\n# models. Here, all stages of Faster R-CNN are instantiated, ground truth\n# labels from RPN proposal boxes (RoIs) for the detector stage are generated,\n# and proposals are sampled.\n#\n\n#\n# Weight Decay\n# ------------\n# Keras does not provide a weight decay option but rather an L2 penalty. Weight\n# decay can be converted to L2 by dividing by 2. This is because the L2 penalty\n# is added to the loss and then differentiated with respect to the weights\n# (introducing a factor of 2 that must be canceled out). See:\n# https://bbabenko.github.io/weight-decay/\n#\n# Pro-Tip\n# -------\n#\n# To log the output of Keras layers using tf.print, use K.Lambda as below:\n#\n#   def do_log1(x):\n#     tf.print(\"best_ious=\", x, output_stream = \"file:///projects/frcnn/tf2/out.txt\", summarize = -1)\n#     return x\n#   best_ious = Lambda(do_log1)(best_ious)\n#\n#   def do_log(x):\n#     y_predicted = x[0]\n#     y_true = x[1]\n#     loss = K.mean(K.categorical_crossentropy(target = y_true, output = y_predicted, from_logits = True))\n#     tf.print(\"loss=\", loss, \"y_predicted=\", y_predicted, output_stream = \"file:///projects/frcnn/tf2/out.txt\", summarize = -1)\n#     return y_predicted\n#   y_predicted = Lambda(do_log)((y_predicted, y_true))\n#\n# output_stream may also be a file stream like sys.stdout.\n#\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Lambda\n\n\nclass FasterRCNNModel(tf.keras.Model):\n  def __init__(self, num_classes, allow_edge_proposals, custom_roi_pool, activate_class_outputs, l2 = 0, dropout_probability = 0):\n    super().__init__()\n    self._num_classes = num_classes\n    self._activate_class_outputs = activate_class_outputs\n    self._stage1_feature_extractor = FeatureExtractor(l2 = l2)\n    self._stage2_region_proposal_network = RegionProposalNetwork(\n      max_proposals_pre_nms_train = 12000,\n      max_proposals_post_nms_train = 2000,\n      max_proposals_pre_nms_infer = 6000,\n      max_proposals_post_nms_infer = 300,\n      l2 = l2,\n      allow_edge_proposals = allow_edge_proposals\n    )\n    self._stage3_detector_network = DetectorNetwork(\n      num_classes = num_classes,\n      custom_roi_pool = custom_roi_pool,\n      activate_class_outputs = activate_class_outputs,\n      l2 = l2,\n      dropout_probability = dropout_probability\n    )\n\n  def call(self, inputs, training = False):\n    # Unpack inputs\n    input_image = inputs[0]             # (1, height_pixels, width_pixels, 3)\n    anchor_map = inputs[1]              # (1, height, width, num_anchors * 4)\n    anchor_valid_map = inputs[2]        # (1, height, width, num_anchors)\n    if training:\n      gt_rpn_map = inputs[3]            # (1, height, width, num_anchors, 6)\n      gt_box_class_idxs_map = inputs[4] # (1, num_gt_boxes)\n      gt_box_corners_map = inputs[5]    # (1, num_gt_boxes, 4)\n\n    # Stage 1: Extract features\n    feature_map = self._stage1_feature_extractor(input_image = input_image, training = training)\n\n    # Stage 2: Generate object proposals using RPN\n    rpn_scores, rpn_box_deltas, proposals = self._stage2_region_proposal_network(\n      inputs = [\n        input_image,\n        feature_map,\n        anchor_map,\n        anchor_valid_map\n      ],\n      training = training\n    )\n\n    # If training, we must generate ground truth data for the detector stage\n    # from RPN outputs\n    if training:\n      # Assign labels to proposals and take random sample (for detector training)\n      proposals, gt_classes, gt_box_deltas = self._label_proposals(\n        proposals = proposals,\n        gt_box_class_idxs = gt_box_class_idxs_map[0], # for now, batch size of 1\n        gt_box_corners = gt_box_corners_map[0],\n        min_background_iou_threshold = 0.0,\n        min_object_iou_threshold = 0.5\n      )\n      proposals, gt_classes, gt_box_deltas = self._sample_proposals(\n        proposals = proposals,\n        gt_classes = gt_classes,\n        gt_box_deltas = gt_box_deltas,\n        max_proposals = 128,\n        positive_fraction = 0.25\n      )\n      gt_classes = tf.expand_dims(gt_classes, axis = 0)           # (N,num_classes) -> (1,N,num_classes) (as expected by loss function)\n      gt_box_deltas = tf.expand_dims(gt_box_deltas, axis = 0)   # (N,2,(num_classes-1)*4) -> (1,N,2,(num_classes-1)*4)\n\n      # Ensure proposals are treated as constants and do not propagate gradients\n      proposals = tf.stop_gradient(proposals)\n      gt_classes = tf.stop_gradient(gt_classes)\n      gt_box_deltas = tf.stop_gradient(gt_box_deltas)\n\n    # Stage 3: Detector\n    detector_classes, detector_box_deltas = self._stage3_detector_network(\n      inputs = [\n        input_image,\n        feature_map,\n        proposals\n      ],\n      training = training\n    )\n\n    # Losses\n    if training:\n      rpn_class_loss = self._stage2_region_proposal_network.class_loss(y_predicted = rpn_scores, gt_rpn_map = gt_rpn_map)\n      rpn_regression_loss = self._stage2_region_proposal_network.regression_loss(y_predicted = rpn_box_deltas, gt_rpn_map = gt_rpn_map)\n      detector_class_loss = self._stage3_detector_network.class_loss(y_predicted = detector_classes, y_true = gt_classes, from_logits = not self._activate_class_outputs)\n      detector_regression_loss = self._stage3_detector_network.regression_loss(y_predicted = detector_box_deltas, y_true = gt_box_deltas)\n      self.add_loss(rpn_class_loss)\n      self.add_loss(rpn_regression_loss)\n      self.add_loss(detector_class_loss)\n      self.add_loss(detector_regression_loss)\n      self.add_metric(rpn_class_loss, name = \"rpn_class_loss\")\n      self.add_metric(rpn_regression_loss, name = \"rpn_regression_loss\")\n      self.add_metric(detector_class_loss, name = \"detector_class_loss\")\n      self.add_metric(detector_regression_loss, name = \"detector_regression_loss\")\n    else:\n      # Losses cannot be computed during inference and should be ignored\n      rpn_class_loss = float(\"inf\")\n      rpn_regression_loss = float(\"inf\")\n      detector_class_loss = float(\"inf\")\n      detector_regression_loss = float(\"inf\")\n\n    # Return outputs\n    return [\n      rpn_scores,\n      rpn_box_deltas,\n      detector_classes,\n      detector_box_deltas,\n      proposals,\n      rpn_class_loss,\n      rpn_regression_loss,\n      detector_class_loss,\n      detector_regression_loss\n   ]\n\n  def predict_on_batch(self, x, score_threshold):\n    \"\"\"\n    Use this method to run inference. Overrides the default Keras\n    implementation to return scored boxes.\n\n    Parameters\n    ----------\n    x : List[np.ndarray]\n      List of input maps, each of batch size 1:\n        - Input image: (1, height_pixels, width_pixels, 3)\n        - Anchor map: (1, height, width, num_anchors * 4)\n        - Anchor valid map: (1, height, width, num_anchors)\n    score_threshold : float\n      Minimum class score for detections. Detections scoring below this value\n      are discarded.\n\n    Returns\n    -------\n    Dict[int, Tuple[float, float, float, float, float]]\n      Scored boxes by class index. Each box is a tuple of\n      (y_min, x_min, y_max, x_max, score).\n    \"\"\"\n    _, _, detector_classes, detector_box_deltas, proposals, _, _, _, _ = super().predict_on_batch(x = x)\n    scored_boxes_by_class_index = self._predictions_to_scored_boxes(\n      input_image = x[0],\n      classes = detector_classes,\n      box_deltas = detector_box_deltas,\n      proposals = proposals,\n      score_threshold = score_threshold\n    )\n    return scored_boxes_by_class_index\n\n  def load_imagenet_weights(self):\n    \"\"\"\n    Load weights from Keras VGG-16 model pre-trained on ImageNet into the\n    feature extractor convolutional layers as well as the two fully connected\n    layers in the detector stage.\n    \"\"\"\n    keras_model = tf.keras.applications.VGG16(weights = \"imagenet\")\n    for keras_layer in keras_model.layers:\n      weights = keras_layer.get_weights()\n      if len(weights) > 0:\n        vgg16_layers = self._stage1_feature_extractor.layers + self._stage3_detector_network.layers\n        our_layer = [ layer for layer in vgg16_layers if layer.name == keras_layer.name ]\n        if len(our_layer) > 0:\n          print(\"Loading VGG-16 ImageNet weights into layer: %s\" % our_layer[0].name)\n          our_layer[0].set_weights(weights)\n\n  def _predictions_to_scored_boxes(self, input_image, classes, box_deltas, proposals, score_threshold):\n    # Eliminate batch dimension\n    input_image = np.squeeze(input_image, axis = 0)\n    classes = np.squeeze(classes, axis = 0)\n    box_deltas = np.squeeze(box_deltas, axis = 0)\n\n    # Convert logits to probability distribution if using logits mode\n    if not self._activate_class_outputs:\n      classes = tf.nn.softmax(classes, axis = 1).numpy()\n\n    # Convert proposal boxes -> center point and size\n    proposal_anchors = np.empty(proposals.shape)\n    proposal_anchors[:,0] = 0.5 * (proposals[:,0] + proposals[:,2]) # center_y\n    proposal_anchors[:,1] = 0.5 * (proposals[:,1] + proposals[:,3]) # center_x\n    proposal_anchors[:,2:4] = proposals[:,2:4] - proposals[:,0:2]   # height, width\n\n    # Separate out results per class: class_idx -> (y1, x1, y2, x2, score)\n    boxes_and_scores_by_class_idx = {}\n    for class_idx in range(1, classes.shape[1]):  # skip class 0 (background)\n      # Get the regression parameters (ty, tx, th, tw) corresponding to this\n      # class, for all proposals\n      box_delta_idx = (class_idx - 1) * 4\n      box_delta_params = box_deltas[:, (box_delta_idx + 0) : (box_delta_idx + 4)] # (N, 4)\n      proposal_boxes_this_class = convert_deltas_to_boxes(\n        box_deltas = box_delta_params,\n        anchors = proposal_anchors,\n        box_delta_means = [0.0, 0.0, 0.0, 0.0],\n        box_delta_stds = [0.1, 0.1, 0.2, 0.2]\n      )\n\n      # Clip to image boundaries\n      proposal_boxes_this_class[:,0::2] = np.clip(proposal_boxes_this_class[:,0::2], 0, input_image.shape[0] - 1) # clip y1 and y2 to [0,height)\n      proposal_boxes_this_class[:,1::2] = np.clip(proposal_boxes_this_class[:,1::2], 0, input_image.shape[1] - 1) # clip x1 and x2 to [0,width)\n\n      # Get the scores for this class. The class scores are returned in\n      # normalized categorical form. Each row corresponds to a class.\n      scores_this_class = classes[:,class_idx]\n\n      # Keep only those scoring high enough\n      sufficiently_scoring_idxs = np.where(scores_this_class > score_threshold)[0]\n      proposal_boxes_this_class = proposal_boxes_this_class[sufficiently_scoring_idxs]\n      scores_this_class = scores_this_class[sufficiently_scoring_idxs]\n      boxes_and_scores_by_class_idx[class_idx] = (proposal_boxes_this_class, scores_this_class)\n\n    # Perform NMS per class\n    scored_boxes_by_class_idx = {}\n    for class_idx, (boxes, scores) in boxes_and_scores_by_class_idx.items():\n      idxs = tf.image.non_max_suppression(\n        boxes = boxes,\n        scores = scores,\n        max_output_size = proposals.shape[0],\n        iou_threshold = 0.3\n      )\n      idxs = idxs.numpy()\n      boxes = boxes[idxs]\n      scores = np.expand_dims(scores[idxs], axis = 0) # (N,) -> (N,1)\n      scored_boxes = np.hstack([ boxes, scores.T ])   # (N,5), with each row: (y1, x1, y2, x2, score)\n      scored_boxes_by_class_idx[class_idx] = scored_boxes\n\n    return scored_boxes_by_class_idx\n\n  def _label_proposals(self, proposals, gt_box_class_idxs, gt_box_corners, min_background_iou_threshold, min_object_iou_threshold):\n    \"\"\"\n    Determines which proposals generated by the RPN stage overlap with ground\n    truth boxes and creates ground truth labels for the subsequent detector\n    stage.\n\n    Parameters\n    ----------\n    proposals : tf.Tensor\n      Proposal corners, shaped (N, 4), where each corner is:\n      (y_min, x_min, y_max, x_max).\n    gt_box_class_idxs : tf.Tensor\n      The class index for each ground truth box, shaped (M,), where M is the\n      number of ground truth boxes.\n    gt_box_corners: tf.Tensor\n      Ground truth box corners, shaped (M, 4).\n    min_background_iou_threshold : float\n      Minimum IoU threshold with ground truth boxes below which proposals are\n      ignored entirely. Proposals with an IoU threshold in the range\n      [min_background_iou_threshold, min_object_iou_threshold) are labeled as\n      background. This value can be greater than 0, which has the effect of\n      selecting more difficult background examples that have some degree of\n      overlap with ground truth boxes.\n    min_object_iou_threshold : float\n      Minimum IoU threshold for a proposal to be labeled as an object.\n\n    Returns\n    -------\n    tf.Tensor, tf.Tensor, tf.Tensor\n      Proposals, (N, 4), labeled as either objects or background (depending on\n      IoU thresholds, some proposals can end up as neither and are excluded\n      here); one-hot encoded class labels, (N, num_classes), for each proposal;\n      and box delta regression targets, (N, 2, (num_classes - 1) * 4), for each\n      proposal. Regression target values are present at locations [:,1,:] and\n      consist of (ty, tx, th, tw) for the class that the box corresponds to.\n      The entries for all other classes and the background classes should be\n      ignored. A mask is written to locations [:,0,:]. For each proposal\n      assigned a non-background class, there will be 4 consecutive elements\n      marked with 1 indicating the corresponding regression target values are\n      to be used. There are no regression targets for background proposals and\n      the mask is entirely 0 for those proposals.\n    \"\"\"\n    # Let's be crafty and create some fake proposals that match the ground\n    # truth boxes exactly. This isn't strictly necessary and the model should\n    # work without it but it will help training and will ensure that there are\n    # always some positive examples to train on.\n    proposals = tf.concat([ proposals, gt_box_corners ], axis = 0)\n\n    # Compute IoU between each proposal (N,4) and each ground truth box (M,4)\n    # -> (N, M)\n    ious = tf_intersection_over_union(boxes1 = proposals, boxes2 = gt_box_corners)\n\n    # Find the best IoU for each proposal, the class of the ground truth box\n    # associated with it, and the box corners\n    best_ious = tf.math.reduce_max(ious, axis = 1)  # (N,) of maximum IoUs for each of the N proposals\n    box_idxs = tf.math.argmax(ious, axis = 1)       # (N,) of ground truth box index for each proposal\n    gt_box_class_idxs = tf.gather(gt_box_class_idxs, indices = box_idxs)  # (N,) of class indices of highest-IoU box for each proposal\n    gt_box_corners = tf.gather(gt_box_corners, indices = box_idxs)        # (N,4) of box corners of highest-IoU box for each proposal\n\n    # Remove all proposals whose best IoU is less than the minimum threshold\n    # for a negative (background) sample. We also check for IoUs > 0 because\n    # due to earlier clipping, we may get invalid 0-area proposals.\n    idxs = tf.where(best_ious >= min_background_iou_threshold)  # keep proposals w/ sufficiently high IoU\n    proposals = tf.gather_nd(proposals, indices = idxs)\n    best_ious = tf.gather_nd(best_ious, indices = idxs)\n    gt_box_class_idxs = tf.gather_nd(gt_box_class_idxs, indices = idxs)\n    gt_box_corners = tf.gather_nd(gt_box_corners, indices = idxs)\n\n    # IoUs less than min_object_iou_threshold will be labeled as background\n    retain_mask = tf.cast(best_ious >= min_object_iou_threshold, dtype = gt_box_class_idxs.dtype) # (N,), with 0 wherever best_iou < threshold, else 1\n    gt_box_class_idxs = gt_box_class_idxs * retain_mask\n\n    # One-hot encode class labels\n    num_classes = self._num_classes\n    gt_classes = tf.one_hot(indices = gt_box_class_idxs, depth = num_classes) # (N,num_classes)\n\n    # Convert proposals and ground truth boxes into \"anchor\" format (center\n    # points and side lengths). For the detector stage, the proposals serve as\n    # the anchors relative to which the final box predictions will be\n    # regressed.\n    proposal_centers = 0.5 * (proposals[:,0:2] + proposals[:,2:4])          # center_y, center_x\n    proposal_sides = proposals[:,2:4] - proposals[:,0:2]                    # height, width\n    gt_box_centers = 0.5 * (gt_box_corners[:,0:2] + gt_box_corners[:,2:4])  # center_y, center_x\n    gt_box_sides = gt_box_corners[:,2:4] - gt_box_corners[:,0:2]            # height, width\n\n    # Compute regression targets (ty, tx, th, tw) for each proposal based on\n    # the best box selected\n    detector_box_delta_means = tf.constant([0, 0, 0, 0], dtype = tf.float32)\n    detector_box_delta_stds = tf.constant([0.1, 0.1, 0.2, 0.2], dtype = tf.float32)\n    tyx = (gt_box_centers - proposal_centers) / proposal_sides  # ty = (gt_center_y - proposal_center_y) / proposal_height, tx = (gt_center_x - proposal_center_x) / proposal_width\n    thw = tf.math.log(gt_box_sides / proposal_sides)            # th = log(gt_height / proposal_height), tw = (gt_width / proposal_width)\n    box_delta_targets = tf.concat([ tyx, thw ], axis = 1)       # (N,4) box delta regression targets tensor\n    box_delta_targets = (box_delta_targets - detector_box_delta_means) / detector_box_delta_stds  # mean and standard deviation adjustment\n\n    # Convert regression targets into a map of shape (N,2,4*(C-1)) where C is\n    # the number of classes and [:,0,:] specifies a mask for the corresponding\n    # target components at [:,1,:]. Targets are ordered (ty, tx, th, tw).\n    # Background class 0 is not present at all.\n    gt_box_deltas_mask = tf.repeat(gt_classes, repeats = 4, axis = 1)[:,4:]             # create masks using interleaved repetition, remembering to discard class 0\n    gt_box_deltas_values = tf.tile(box_delta_targets, multiples = [1, num_classes - 1]) # populate regression targets with straightforward repetition of each row (only those columns corresponding to class will be masked on)\n    gt_box_deltas_mask = tf.expand_dims(gt_box_deltas_mask, axis = 0)     # (N,4*(C-1)) -> (1,N,4*(C-1))\n    gt_box_deltas_values = tf.expand_dims(gt_box_deltas_values, axis = 0) # (N,4*(C-1)) -> (1,N,4*(C-1))\n    gt_box_deltas = tf.concat([ gt_box_deltas_mask, gt_box_deltas_values ], axis = 0) # (2,N,4*(C-1))\n    gt_box_deltas = tf.transpose(gt_box_deltas, perm = [ 1, 0, 2])        # (N,2,4*(C-1))\n\n    return proposals, gt_classes, gt_box_deltas\n\n  def _sample_proposals(self, proposals, gt_classes, gt_box_deltas, max_proposals, positive_fraction):\n    if max_proposals <= 0:\n      return proposals, gt_classes, gt_box_deltas\n\n    # Get positive and negative (background) proposals\n    class_indices = tf.argmax(gt_classes, axis = 1) # (N,num_classes) -> (N,), where each element is the class index (highest score from its row)\n    positive_indices = tf.squeeze(tf.where(class_indices > 0), axis = 1)  # (P,), tensor of P indices (the positive, non-background classes in class_indices)\n    negative_indices = tf.squeeze(tf.where(class_indices <= 0), axis = 1) # (N,), tensor of N indices (the negative, background classes in class_indices)\n    num_positive_proposals = tf.size(positive_indices)\n    num_negative_proposals = tf.size(negative_indices)\n\n    # Select positive and negative samples, if there are enough. Note that the\n    # number of positive samples can be either the positive fraction of the\n    # *actual* number of proposals *or* the *desired* number (max_proposals).\n    # In practice, these yield virtually identical results but the latter\n    # method will yield slightly more positive samples in the rare cases when\n    # the number of proposals is below the desired number. Here, we use the\n    # former method but others, such as Yun Chen, use the latter. To implement\n    # it, replace num_samples with max_proposals in the line that computes\n    # num_positive_samples. I am not sure what the original Faster R-CNN\n    # implementation does.\n    num_samples = tf.minimum(max_proposals, tf.size(class_indices))\n    num_positive_samples = tf.minimum(tf.cast(tf.math.round(tf.cast(num_samples, dtype = float) * positive_fraction), dtype = num_samples.dtype), num_positive_proposals)\n    num_negative_samples = tf.minimum(num_samples - num_positive_samples, num_negative_proposals)\n\n    # Sample randomly\n    positive_sample_indices = tf.random.shuffle(positive_indices)[:num_positive_samples]\n    negative_sample_indices = tf.random.shuffle(negative_indices)[:num_negative_samples]\n    indices = tf.concat([ positive_sample_indices, negative_sample_indices ], axis = 0)\n\n    # My initial PyTorch version was careful to return empty tensors if there\n    # were no positive samples or no negative samples. Because TF2/Keras is awful\n    # and tf.cond doesn't work due to some incompatibility between tf.function\n    # and KerasTensor, we always return the proposals even if there are no\n    # negative samples among them. Ths occurs very rarely. Positive samples are\n    # guaranteed to exist because _label_proposals inserts the ground truth boxes\n    # as fake proposals to boost learning.\n    \"\"\"\n    # Do we have enough?\n    no_samples = tf.logical_or(tf.math.less_equal(num_positive_samples, 0), tf.math.less_equal(num_negative_samples, 0))\n\n    # Return (if we have any samples)\n    proposals = tf.cond(\n      no_samples,\n      true_fn = lambda: tf.zeros(shape = (0, 4), dtype = proposals.dtype),  # empty proposals tensor if no samples\n      false_fn = lambda: tf.gather(proposals, indices = indices)            # gather samples\n    )\n    gt_classes = tf.cond(\n      no_samples,\n      true_fn = lambda: tf.zeros(shape = (0, tf.shape(gt_classes)[1]), dtype = gt_classes.dtype), # empty list of classes if no samples\n      false_fn = lambda: tf.gather(gt_classes, indices = indices)                                 # gather samples\n    )\n    gt_box_deltas = tf.cond(\n      no_samples,\n      true_fn = lambda: tf.zeros(shape = (0, tf.shape(gt_box_deltas)[1]), dtype = gt_box_deltas.dtype), # empty list of classes if no samples\n      false_fn = lambda: tf.gather(gt_box_deltas, indices = indices)                                     # gather samples\n    )\n    \"\"\"\n\n    return tf.gather(proposals, indices = indices), tf.gather(gt_classes, indices = indices), tf.gather(gt_box_deltas, indices = indices)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:52.867532Z","iopub.execute_input":"2022-04-24T04:52:52.867908Z","iopub.status.idle":"2022-04-24T04:52:53.078648Z","shell.execute_reply.started":"2022-04-24T04:52:52.867877Z","shell.execute_reply":"2022-04-24T04:52:53.077905Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Options:\n    def __init__(self,):\n        self.train = True\n        self.eval = False\n        self.predict = False\n        self.predict_to_file = \"\"\n        self.predict_all = \"\"\n        self.dataset_dir = \"/kaggle/input/chess-object-detection/chess-pieces-dataset/\"\n        self.load_from = \"fasterrcnn_tf2.h5\" # \"fasterrcnn_tf2.h5\"\n        self.save_to = \"fasterrcnn_tf2.h5\"\n        self.save_best_to = \"fasterrcnn_tf2.h5\"\n        self.train_split = \"train\"\n        self.eval_split = \"test\"\n        self.cache_images = False\n        self.periodic_eval_samples = False\n        self.checkpoint_dir = False\n        self.plot = False\n        self.log_csv = \"log.csv\"\n        self.optimizer = \"adam\"\n        self.learning_rate = 1e-3\n        self.epochs = 10\n        self.clipnorm = 0.0\n        self.momentum = 0.9\n        self.beta1 = 0.9\n        self.beta2 = 0.999\n        self.weight_decay = 5e-4\n        self.dropout = 0.0\n        self.custom_roi_pool = 0.0\n        self.detector_logits = False\n        self.detector_logits = False\n        self.no_augment = True\n        self.exclude_edge_proposals = True\n        self.dump_anchors = True\n        self.debug_dir = \"\"\n\n\noptions = Options()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T05:00:19.247606Z","iopub.execute_input":"2022-04-24T05:00:19.248421Z","iopub.status.idle":"2022-04-24T05:00:19.259494Z","shell.execute_reply.started":"2022-04-24T05:00:19.248373Z","shell.execute_reply":"2022-04-24T05:00:19.258675Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Run-time environment\ncuda_available = tf.test.is_built_with_cuda()\ngpu_available = tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\nprint(\"CUDA Available : %s\" % (\"yes\" if cuda_available else \"no\"))\nprint(\"GPU Available  : %s\" % (\"yes\" if gpu_available else \"no\"))\nprint(\"Eager Execution: %s\" % (\"yes\" if tf.executing_eagerly() else \"no\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:53.093941Z","iopub.execute_input":"2022-04-24T04:52:53.094406Z","iopub.status.idle":"2022-04-24T04:52:55.007315Z","shell.execute_reply.started":"2022-04-24T04:52:53.094370Z","shell.execute_reply":"2022-04-24T04:52:55.006425Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"CUDA Available : yes\nGPU Available  : yes\nEager Execution: yes\n","output_type":"stream"},{"name":"stderr","text":"2022-04-24 04:52:53.101337: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-04-24 04:52:53.156667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-04-24 04:52:53.245568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-04-24 04:52:53.246316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-04-24 04:52:54.994946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-04-24 04:52:54.995838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-04-24 04:52:54.996478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-04-24 04:52:54.997064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"def render_anchors():\n    training_data = Dataset(dir=options.dataset_dir, split=options.train_split, augment=False, shuffle=False)\n    if not os.path.exists(options.dump_anchors):\n        os.makedirs(options.dump_anchors)\n    print(\"Rendering anchors from '%s' to set '%s'...\" % (options.train_split, options.dump_anchors))\n    for sample in iter(training_data):\n        output_path = os.path.join(options.dump_anchors, \"anchors_\" + os.path.basename(sample.filepath) + \".png\")\n        show_anchors(\n            output_path=output_path,\n            image=sample.image,\n            anchor_map=sample.anchor_map,\n            anchor_valid_map=sample.anchor_valid_map,\n            gt_rpn_map=sample.gt_rpn_map,\n            gt_boxes=sample.gt_boxes\n        )","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:55.008445Z","iopub.execute_input":"2022-04-24T04:52:55.009030Z","iopub.status.idle":"2022-04-24T04:52:55.019249Z","shell.execute_reply.started":"2022-04-24T04:52:55.008992Z","shell.execute_reply":"2022-04-24T04:52:55.018517Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# # Perform optional procedures\n# if options.dump_anchors:\n#     render_anchors()\n\n# Debug logging\nif options.debug_dir:\n    tf.debugging.experimental.enable_dump_debug_info(options.debug_dir, tensor_debug_mode=\"FULL_HEALTH\",\n                                                     circular_buffer_size=-1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:55.020420Z","iopub.execute_input":"2022-04-24T04:52:55.021074Z","iopub.status.idle":"2022-04-24T04:52:55.028464Z","shell.execute_reply.started":"2022-04-24T04:52:55.021038Z","shell.execute_reply":"2022-04-24T04:52:55.027683Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class CSVLog:\n  \"\"\"\n  Logs to a CSV file.\n  \"\"\"\n  def __init__(self, filename):\n    self._filename = filename\n    self._header_written = False\n\n  def log(self, items):\n    keys = list(items.keys())\n    file_mode = \"a\" if self._header_written else \"w\"\n    with open(self._filename, file_mode) as fp:\n      if not self._header_written:\n        fp.write(\",\".join(keys) + \"\\n\")\n        self._header_written = True\n      values = [ str(value) for (key, value) in items.items() ]\n      fp.write(\",\".join(values) + \"\\n\")\n\nclass BestWeightsTracker:\n  def __init__(self, filepath):\n    self._filepath = filepath\n    self._best_weights = None\n    self._best_mAP = 0\n\n  def on_epoch_end(self, model, mAP):\n    if mAP > self._best_mAP:\n      self._best_mAP = mAP\n      self._best_weights = model.get_weights()\n\n  def restore_and_save_best_weights(self, model):\n    if self._best_weights is not None:\n      model.set_weights(self._best_weights)\n      model.save_weights(filepath = self._filepath, overwrite = True, save_format = \"h5\")\n      print(\"Saved best model weights (Mean Average Precision = %1.2f%%) to '%s'\" % (self._best_mAP, self._filepath))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:55.029980Z","iopub.execute_input":"2022-04-24T04:52:55.030632Z","iopub.status.idle":"2022-04-24T04:52:55.041865Z","shell.execute_reply.started":"2022-04-24T04:52:55.030508Z","shell.execute_reply":"2022-04-24T04:52:55.041073Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_optimizer():\n    kwargs = {}\n    if options.clipnorm > 0:\n        kwargs = {\"clipnorm\": options.clipnorm}\n    if options.optimizer == \"sgd\":\n        optimizer = SGD(learning_rate=options.learning_rate, momentum=options.momentum, **kwargs)\n    elif options.optimizer == \"adam\":\n        optimizer = Adam(learning_rate=options.learning_rate, beta_1=options.beta1, beta_2=options.beta2, **kwargs)\n    else:\n        raise ValueError(\"Optimizer must be 'sgd' for stochastic gradient descent or 'adam' for Adam\")\n    return optimizer\n\ndef _sample_rpn_minibatch(rpn_map, object_indices, background_indices, rpn_minibatch_size):\n    \"\"\"\n    Selects anchors for training and produces a copy of the RPN ground truth\n    map with only those anchors marked as trainable.\n\n    Parameters\n    ----------\n    rpn_map : np.ndarray\n      RPN ground truth map of shape\n      (batch_size, height, width, num_anchors, 6).\n    object_indices : List[np.ndarray]\n      For each image in the batch, a map of shape (N, 3) of indices (y, x, k)\n      of all N object anchors in the RPN ground truth map.\n    background_indices : List[np.ndarray]\n      For each image in the batch, a map of shape (M, 3) of indices of all M\n      background anchors in the RPN ground truth map.\n\n    Returns\n    -------\n    np.ndarray\n      A copy of the RPN ground truth map with index 0 of the last dimension\n      recomputed to include only anchors in the minibatch.\n    \"\"\"\n    assert rpn_map.shape[0] == 1, \"Batch size must be 1\"\n    assert len(object_indices) == 1, \"Batch size must be 1\"\n    assert len(background_indices) == 1, \"Batch size must be 1\"\n    positive_anchors = object_indices[0]\n    negative_anchors = background_indices[0]\n    assert len(positive_anchors) + len(\n        negative_anchors) >= rpn_minibatch_size, \"Image has insufficient anchors for RPN minibatch size of %d\" % rpn_minibatch_size\n    assert len(positive_anchors) > 0, \"Image does not have any positive anchors\"\n    assert rpn_minibatch_size % 2 == 0, \"RPN minibatch size must be evenly divisible\"\n\n    # Sample, producing indices into the index maps\n    num_positive_anchors = len(positive_anchors)\n    num_negative_anchors = len(negative_anchors)\n    num_positive_samples = min(rpn_minibatch_size // 2,\n                               num_positive_anchors)  # up to half the samples should be positive, if possible\n    num_negative_samples = rpn_minibatch_size - num_positive_samples  # the rest should be negative\n    positive_anchor_idxs = random.sample(range(num_positive_anchors), num_positive_samples)\n    negative_anchor_idxs = random.sample(range(num_negative_anchors), num_negative_samples)\n\n    # Construct index expressions into RPN map\n    positive_anchors = positive_anchors[positive_anchor_idxs]\n    negative_anchors = negative_anchors[negative_anchor_idxs]\n    trainable_anchors = np.concatenate([positive_anchors, negative_anchors])\n    batch_idxs = np.zeros(len(trainable_anchors), dtype=int)\n    trainable_idxs = (batch_idxs, trainable_anchors[:, 0], trainable_anchors[:, 1], trainable_anchors[:, 2], 0)\n\n    # Create a copy of the RPN map with samples set as trainable\n    rpn_minibatch_map = rpn_map.copy()\n    rpn_minibatch_map[:, :, :, :, 0] = 0\n    rpn_minibatch_map[trainable_idxs] = 1\n\n    return rpn_minibatch_map","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:55.044708Z","iopub.execute_input":"2022-04-24T04:52:55.044907Z","iopub.status.idle":"2022-04-24T04:52:55.061559Z","shell.execute_reply.started":"2022-04-24T04:52:55.044881Z","shell.execute_reply":"2022-04-24T04:52:55.060811Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def _convert_training_sample_to_model_input(sample, mode):\n    \"\"\"\n    Converts a training sample obtained from the dataset into an input vector\n    that can be passed to the model.\n\n    Parameters\n    ----------\n    sample : datasets.training_sample.TrainingSample\n      Training sample obtained from dataset.\n    mode : str\n      \"train\" if the input vector will be fed into a training model otherwise\n      \"infer\".\n\n    Returns\n    -------\n    List[np.ndarray], np.ndarray, np.ndarray\n      Input vector for model (see model definition for details), image data,\n      and ground truth RPN minibatch map. All maps are converted to a batch\n      size of 1 as expected by Keras model.\n    \"\"\"\n\n    # Ground truth boxes to NumPy arrays\n    gt_box_corners = np.array([box.corners for box in sample.gt_boxes]).astype(\n        np.float32)  # (num_boxes,4), where each row is (y1,x1,y2,x2)\n    gt_box_class_idxs = np.array([box.class_index for box in sample.gt_boxes]).astype(\n        np.int32)  # (num_boxes,), where each is an index [1,num_classes)\n\n    # Expand all maps to a batch size of 1\n    image_data = np.expand_dims(sample.image_data, axis=0)\n    image_shape_map = np.array(\n        [[image_data.shape[1], image_data.shape[2], image_data.shape[3]]])  # (1,3), with (height,width,channels)\n    anchor_map = np.expand_dims(sample.anchor_map, axis=0)\n    anchor_valid_map = np.expand_dims(sample.anchor_valid_map, axis=0)\n    gt_rpn_map = np.expand_dims(sample.gt_rpn_map, axis=0)\n    gt_rpn_object_indices = [sample.gt_rpn_object_indices]\n    gt_rpn_background_indices = [sample.gt_rpn_background_indices]\n    gt_box_corners = np.expand_dims(gt_box_corners, axis=0)\n    gt_box_class_idxs = np.expand_dims(gt_box_class_idxs, axis=0)\n\n    # Create a RPN minibatch: sample anchors randomly and create a new ground\n    # truth RPN map\n    gt_rpn_minibatch_map = _sample_rpn_minibatch(\n        rpn_map=gt_rpn_map,\n        object_indices=gt_rpn_object_indices,\n        background_indices=gt_rpn_background_indices,\n        rpn_minibatch_size=256\n    )\n\n    # Input vector to model\n    if mode == \"train\":\n        x = [image_data, anchor_map, anchor_valid_map, gt_rpn_minibatch_map, gt_box_class_idxs, gt_box_corners]\n    else:  # \"infer\"\n        x = [image_data, anchor_map, anchor_valid_map]\n\n    # Return all plus some unpacked elements for convenience\n    return x, image_data, gt_rpn_minibatch_map\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:55.063795Z","iopub.execute_input":"2022-04-24T04:52:55.064241Z","iopub.status.idle":"2022-04-24T04:52:55.076544Z","shell.execute_reply.started":"2022-04-24T04:52:55.064190Z","shell.execute_reply":"2022-04-24T04:52:55.075708Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, eval_data=None, num_samples=None, plot=False, print_average_precisions=False):\n    if eval_data is None:\n        eval_data = Dataset(dir=options.dataset_dir, split=options.eval_split, augment=False, shuffle=False)\n    if num_samples is None:\n        num_samples = eval_data.num_samples\n    precision_recall_curve = PrecisionRecallCurveCalculator()\n    i = 0\n    print(\"Evaluating '%s'...\" % eval_data.split)\n    for sample in tqdm(iterable=iter(eval_data), total=num_samples):\n        x, image_data, _ = _convert_training_sample_to_model_input(sample=sample, mode=\"infer\")\n        scored_boxes_by_class_index = model.predict_on_batch(x=x,\n                                                             score_threshold=0.05)  # lower threshold score for evaluation\n        precision_recall_curve.add_image_results(\n            scored_boxes_by_class_index=scored_boxes_by_class_index,\n            gt_boxes=sample.gt_boxes\n        )\n        i += 1\n        if i >= num_samples:\n            break\n    if print_average_precisions:\n        precision_recall_curve.print_average_precisions(class_index_to_name=Dataset.class_index_to_name)\n    mean_average_precision = 100.0 * precision_recall_curve.compute_mean_average_precision()\n    print(\"Mean Average Precision = %1.2f%%\" % mean_average_precision)\n    if plot:\n        precision_recall_curve.plot_average_precisions(class_index_to_name=Dataset.class_index_to_name)\n    return mean_average_precision","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:55.078306Z","iopub.execute_input":"2022-04-24T04:52:55.078552Z","iopub.status.idle":"2022-04-24T04:52:55.091215Z","shell.execute_reply.started":"2022-04-24T04:52:55.078526Z","shell.execute_reply":"2022-04-24T04:52:55.090357Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(model):\n    print(\"Training Parameters\")\n    print(\"-------------------\")\n    print(\"Initial weights           : %s\" % (\n        options.load_from if options.load_from else \"Keras VGG-16 ImageNet weights\"))\n    print(\"Dataset                   : %s\" % options.dataset_dir)\n    print(\"Training split            : %s\" % options.train_split)\n    print(\"Evaluation split          : %s\" % options.eval_split)\n    print(\"Epochs                    : %d\" % options.epochs)\n    print(\"Optimizer                 : %s\" % options.optimizer)\n    print(\"Learning rate             : %f\" % options.learning_rate)\n    print(\"Gradient norm clipping    : %s\" % (\"disabled\" if options.clipnorm <= 0 else (\"%f\" % options.clipnorm)))\n    print(\"SGD momentum              : %f\" % options.momentum)\n    print(\"Adam Beta-1               : %f\" % options.beta1)\n    print(\"Adam Beta-2               : %f\" % options.beta2)\n    print(\"Weight decay              : %f\" % options.weight_decay)\n    print(\"Dropout                   : %f\" % options.dropout)\n    print(\"RoI pooling implementation: %s\" % (\"custom\" if options.custom_roi_pool else \"crop-and-resize w/ max pool\"))\n    print(\"Detector output           : %s\" % (\"logits\" if options.detector_logits else \"probabilities\"))\n    print(\"Augmentation              : %s\" % (\"disabled\" if options.no_augment else \"enabled\"))\n    print(\"Edge proposals            : %s\" % (\"excluded\" if options.exclude_edge_proposals else \"included\"))\n    print(\"CSV log                   : %s\" % (\"none\" if not options.log_csv else options.log_csv))\n    print(\"Checkpoints               : %s\" % (\"disabled\" if not options.checkpoint_dir else options.checkpoint_dir))\n    print(\"Final weights file        : %s\" % (\"none\" if not options.save_to else options.save_to))\n    print(\"Best weights file         : %s\" % (\"none\" if not options.save_best_to else options.save_best_to))\n    # training_data = Dataset(dir=options.dataset_dir, split=options.train_split, augment=not options.no_augment,\n    #                             shuffle=True, cache=options.cache_images)\n    training_data = Dataset(dir=options.dataset_dir, split=options.train_split, augment=not options.no_augment,\n                                shuffle=True, cache=False)\n    eval_data = Dataset(dir=options.dataset_dir, split=options.eval_split, augment=False, shuffle=False,\n                            cache=False)\n    if options.checkpoint_dir and not os.path.exists(options.checkpoint_dir):\n        os.makedirs(options.checkpoint_dir)\n    if options.log_csv:\n        csv = CSVLog(options.log_csv)\n    if options.save_best_to:\n        best_weights_tracker = BestWeightsTracker(filepath=options.save_best_to)\n    for epoch in range(1, 1 + options.epochs):\n        print(\"Epoch %d/%d\" % (epoch, options.epochs))\n        stats = TrainingStatistics()\n        progbar = tqdm(iterable=iter(training_data), total=training_data.num_samples,\n                       postfix=stats.get_progbar_postfix())\n        for sample in progbar:\n            x, image_data, gt_rpn_minibatch_map = _convert_training_sample_to_model_input(sample=sample, mode=\"train\")\n            losses = model.train_on_batch(x=x, y=gt_rpn_minibatch_map, return_dict=True)\n            stats.on_training_step(losses=losses)\n            progbar.set_postfix(stats.get_progbar_postfix())\n        last_epoch = epoch == options.epochs\n        mean_average_precision = evaluate(\n            model=model,\n            eval_data=eval_data,\n            num_samples=options.periodic_eval_samples,\n            plot=False,\n            print_average_precisions=False\n        )\n        if options.checkpoint_dir:\n            checkpoint_file = os.path.join(options.checkpoint_dir,\n                                           \"checkpoint-epoch-%d-mAP-%1.1f.h5\" % (epoch, mean_average_precision))\n            model.save_weights(filepath=checkpoint_file, overwrite=True, save_format=\"h5\")\n            print(\"Saved model checkpoint to '%s'\" % checkpoint_file)\n        if options.log_csv:\n            log_items = {\n                \"epoch\": epoch,\n                \"learning_rate\": options.learning_rate,\n                \"clipnorm\": options.clipnorm,\n                \"momentum\": options.momentum,\n                \"beta1\": options.beta1,\n                \"beta2\": options.beta2,\n                \"weight_decay\": options.weight_decay,\n                \"dropout\": options.dropout,\n                \"mAP\": mean_average_precision\n            }\n            log_items.update(stats.get_progbar_postfix())\n            csv.log(log_items)\n        if options.save_best_to:\n            best_weights_tracker.on_epoch_end(model=model, mAP=mean_average_precision)\n    if options.save_to:\n        model.save_weights(filepath=options.save_to, overwrite=True, save_format=\"h5\")\n        print(\"Saved final model weights to '%s'\" % options.save_to)\n    if options.save_best_to:\n        best_weights_tracker.restore_and_save_best_weights(model=model)\n    print(\"Evaluating %s model on all samples in '%s'...\" % (\n        (\"best\" if options.save_best_to else \"final\"),\n        options.eval_split))  # evaluate final or best model on full dataset\n    evaluate(\n        model=model,\n        eval_data=eval_data,\n        num_samples=eval_data.num_samples,\n        plot=options.plot,\n        print_average_precisions=True\n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:52:55.092859Z","iopub.execute_input":"2022-04-24T04:52:55.093465Z","iopub.status.idle":"2022-04-24T04:52:55.117890Z","shell.execute_reply.started":"2022-04-24T04:52:55.093428Z","shell.execute_reply":"2022-04-24T04:52:55.117055Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Construct model and load initial weights\nmodel = FasterRCNNModel(\n    num_classes=Dataset.num_classes,\n    allow_edge_proposals=not options.exclude_edge_proposals,\n    custom_roi_pool=options.custom_roi_pool,\n    activate_class_outputs=not options.detector_logits,\n    l2=0.5 * options.weight_decay,\n    dropout_probability=options.dropout\n)\nmodel.build(\n    input_shape=[\n        (1, None, None, 3),  # input_image: (1, height_pixels, width_pixels, 3)\n        (1, None, None, 9 * 4),  # anchor_map: (1, height, width, num_anchors * 4)\n        (1, None, None, 9),  # anchor_valid_map: (1, height, width, num_anchors)\n        (1, None, None, 9, 6),  # gt_rpn_map: (1, height, width, num_anchors, 6)\n        (1, None),  # gt_box_class_idxs_map: (1, num_gt_boxes)\n        (1, None, 4)  # gt_box_corners_map: (1, num_gt_boxes, 4)\n    ]\n)\nmodel.compile(\n    optimizer=create_optimizer())  # losses not needed here because they were baked in at model construction\nif options.load_from:\n    model.load_weights(filepath=options.load_from, by_name=True)\n    print(\"Loaded initial weights from '%s'\" % options.load_from)\nelse:\n    model.load_imagenet_weights()\n    print(\"Initialized VGG-16 layers to Keras ImageNet weights\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T05:00:32.780500Z","iopub.execute_input":"2022-04-24T05:00:32.780776Z","iopub.status.idle":"2022-04-24T05:00:33.935953Z","shell.execute_reply.started":"2022-04-24T05:00:32.780748Z","shell.execute_reply":"2022-04-24T05:00:33.935265Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Loaded initial weights from 'fasterrcnn_tf2.h5'\n","output_type":"stream"}]},{"cell_type":"code","source":"options.epochs = 5\n# Perform mutually exclusive procedures\nif options.train:\n    train(model=model)\nelif options.eval:\n    evaluate(model=model, plot=options.plot, print_average_precisions=True)\nelif options.predict:\n    predict_one(model=model, url=options.predict, show_image=True, output_path=None)\nelif options.predict_to_file:\n    predict_one(model=model, url=options.predict_to_file, show_image=False, output_path=\"predictions.png\")\nelif options.predict_all:\n    predict_all(model=model, split=options.predict_all)\nelif not options.dump_anchors:\n    print(\"Nothing to do. Did you mean to use --train or --predict?\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T05:01:13.283557Z","iopub.execute_input":"2022-04-24T05:01:13.283900Z","iopub.status.idle":"2022-04-24T05:08:30.187292Z","shell.execute_reply.started":"2022-04-24T05:01:13.283860Z","shell.execute_reply":"2022-04-24T05:08:30.186493Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Training Parameters\n-------------------\nInitial weights           : fasterrcnn_tf2.h5\nDataset                   : /kaggle/input/chess-object-detection/chess-pieces-dataset/\nTraining split            : train\nEvaluation split          : test\nEpochs                    : 5\nOptimizer                 : adam\nLearning rate             : 0.001000\nGradient norm clipping    : disabled\nSGD momentum              : 0.900000\nAdam Beta-1               : 0.900000\nAdam Beta-2               : 0.999000\nWeight decay              : 0.000500\nDropout                   : 0.000000\nRoI pooling implementation: crop-and-resize w/ max pool\nDetector output           : probabilities\nAugmentation              : disabled\nEdge proposals            : excluded\nCSV log                   : log.csv\nCheckpoints               : disabled\nFinal weights file        : fasterrcnn_tf2.h5\nBest weights file         : fasterrcnn_tf2.h5\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 606/606 [01:33<00:00,  6.49it/s, rpn_class_loss=0.1084, rpn_regr_loss=0.0562, detector_class_loss=0.1641, detector_regr_loss=0.1220, total_loss=0.45]\n","output_type":"stream"},{"name":"stdout","text":"Evaluating 'test'...\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Mean Average Precision = 84.72%\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 606/606 [01:23<00:00,  7.24it/s, rpn_class_loss=0.0931, rpn_regr_loss=0.0501, detector_class_loss=0.1382, detector_regr_loss=0.1097, total_loss=0.39]\n","output_type":"stream"},{"name":"stdout","text":"Evaluating 'test'...\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Mean Average Precision = 81.60%\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 606/606 [01:23<00:00,  7.24it/s, rpn_class_loss=0.0869, rpn_regr_loss=0.0473, detector_class_loss=0.1249, detector_regr_loss=0.1091, total_loss=0.37]\n","output_type":"stream"},{"name":"stdout","text":"Evaluating 'test'...\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Mean Average Precision = 63.11%\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 606/606 [01:23<00:00,  7.27it/s, rpn_class_loss=0.0865, rpn_regr_loss=0.0474, detector_class_loss=0.1313, detector_regr_loss=0.1082, total_loss=0.37]\n","output_type":"stream"},{"name":"stdout","text":"Evaluating 'test'...\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Mean Average Precision = 74.31%\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 606/606 [01:23<00:00,  7.27it/s, rpn_class_loss=0.0814, rpn_regr_loss=0.0452, detector_class_loss=0.1201, detector_regr_loss=0.1025, total_loss=0.35]\n","output_type":"stream"},{"name":"stdout","text":"Evaluating 'test'...\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Mean Average Precision = 89.58%\nSaved final model weights to 'fasterrcnn_tf2.h5'\nSaved best model weights (Mean Average Precision = 89.58%) to 'fasterrcnn_tf2.h5'\nEvaluating best model on all samples in 'test'...\nEvaluating 'test'...\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▋| 27/28 [00:02<00:00,  9.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Precisions\n------------------\nblack-king  : 100.0%\nwhite-king  : 99.6%\nwhite-queen : 99.5%\nblack-pawn  : 96.1%\nwhite-pawn  : 93.0%\nblack-queen : 92.9%\nwhite-knight: 90.4%\nblack-bishop: 88.1%\nblack-knight: 87.5%\nwhite-bishop: 86.1%\nwhite-rook  : 77.0%\nblack-rook  : 74.2%\n------------------\nMean Average Precision = 90.36%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Tensorflow\", tensorflow.__version__)\n# print(\"Keras\", keras.__version__)\n# print(\"Keras Pre-processing\", keras_preprocessing.__version__)\nprint(\"Numpy\", np.version)\nprint(\"Pandas\", pd.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:54:05.376359Z","iopub.status.idle":"2022-04-24T04:54:05.376776Z","shell.execute_reply.started":"2022-04-24T04:54:05.376540Z","shell.execute_reply":"2022-04-24T04:54:05.376563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp ../input/chess-vision-model/chess-vision-model/fasterrcnn_tf2.h5 ./fasterrcnn_tf2.h5","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:59:54.465694Z","iopub.execute_input":"2022-04-24T04:59:54.465968Z","iopub.status.idle":"2022-04-24T04:59:59.693117Z","shell.execute_reply.started":"2022-04-24T04:59:54.465937Z","shell.execute_reply":"2022-04-24T04:59:59.692141Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Download best model:\n\n<a href=\"./fasterrcnn_tf2.h5\"> Download File </a>","metadata":{}}]}